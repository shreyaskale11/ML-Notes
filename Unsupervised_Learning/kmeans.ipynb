{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Kmeans\n",
    "\n",
    "\n",
    "take random guesses where the center on cluster will be and then it will go through all example and check whether it \n",
    "is closer to cluster 1 point or cluster 2. then it will look at all of the cluster 1 points and take an average of them. \n",
    "And it will move the cluster 1 centroid point to whatever is the average location of the cluster 1 dots,and do same with \n",
    "cluster 2.and repeat this process till there is no change in cluster's centroid point\n",
    "\n",
    "Sure, here's the provided information rewritten in Markdown format:\n",
    "\n",
    "## K-means Algorithm\n",
    "\n",
    "The K-means algorithm is a technique used for automatically clustering similar data points together. It aims to group a given training set, denoted as $\\{x^{(1)}, ..., x^{(m)}\\}$, into cohesive \"clusters.\" The K-means algorithm operates iteratively and consists of the following steps:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Choose the number of clusters, denoted as `K`.\n",
    "   - Initialize `centroids` as the initial guess for cluster centers.\n",
    "\n",
    "2. **Iteration**:\n",
    "   - Repeat the following steps for a specified number of iterations:\n",
    "     - Cluster Assignment Step:\n",
    "       - For each data point `x`, assign it to the closest centroid. The index `idx[i]` corresponds to the index of the centroid assigned to example `i`.\n",
    "     - Move Centroid Step:\n",
    "       - Recompute the centroids based on the assignments. Update the `centroids` with the mean location of data points assigned to each cluster.\n",
    "\n",
    "3. **Convergence**:\n",
    "   - The K-means algorithm will continue to iterate until there is no significant change in the positions of the centroids.\n",
    "\n",
    "4. **Choosing the Best Solution**:\n",
    "   - The final clustering solution depends on the initial centroids, and different random initializations may lead to different results.\n",
    "   - To choose the best solution, you can run the K-means algorithm multiple times with different initializations and select the one with the lowest cost function value, often referred to as \"distortion.\"\n",
    "\n",
    "The K-means algorithm is a versatile clustering technique that can be applied to various datasets, and it typically converges to a final set of centroid locations. However, the quality of the solution may vary based on the initial centroid guesses.\n",
    "\n",
    "Here's the pseudocode representation of the K-means algorithm:\n",
    "\n",
    "```python\n",
    "# Initialize centroids\n",
    "# K is the number of clusters\n",
    "centroids = kMeans_init_centroids(X, K)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    # Cluster assignment step: \n",
    "    # Assign each data point to the closest centroid. \n",
    "    # idx[i] corresponds to the index of the centroid \n",
    "    # assigned to example i\n",
    "    idx = find_closest_centroids(X, centroids)\n",
    "\n",
    "    # Move centroid step: \n",
    "    # Compute means based on centroid assignments\n",
    "    centroids = compute_centroids(X, idx, K)\n",
    "```\n",
    "\n",
    "In practice, K-means is often run with different random initializations to ensure robust clustering results. The choice of the best solution is determined by minimizing the cost function (distortion).\n",
    "\n",
    "Your task is to complete the code in `find_closest_centroids`. \n",
    "* This function takes the data matrix `X` and the locations of all\n",
    "centroids inside `centroids` \n",
    "* It should output a one-dimensional array `idx` (which has the same number of elements as `X`) that holds the index  of the closest centroid (a value in $\\{0,...,K-1\\}$, where $K$ is total number of centroids) to every training example . *(Note: The index range 0 to K-1 varies slightly from what is shown in the lectures (i.e. 1 to K) because Python list indices start at 0 instead of 1)*\n",
    "* Specifically, for every example $x^{(i)}$ we set\n",
    "$$c^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,$$\n",
    "where \n",
    " * $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$ (corresponds to `idx[i]` in the starter code), and \n",
    " * $\\mu_j$ is the position (value) of the $j$â€™th centroid. (stored in `centroids` in the starter code)\n",
    " * $||x^{(i)} - \\mu_j||$ is the L2-norm\n",
    " \n",
    " Please complete the `compute_centroids` below to recompute the value for each centroid\n",
    "\n",
    "* Specifically, for every centroid $\\mu_k$ we set\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}$$ \n",
    "\n",
    "    where \n",
    "    * $C_k$ is the set of examples that are assigned to centroid $k$\n",
    "    * $|C_k|$ is the number of examples in the set $C_k$\n",
    "\n",
    "\n",
    "* Concretely, if two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $k=2$,\n",
    "then you should update $\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})$.\n",
    "\n",
    "\n",
    "\n",
    "# Random initilization \n",
    "\n",
    "for i in 100{ \n",
    "   randomly initilize k means\n",
    "   run k-means.get $c^{(i)},...,c^{(m)},\\mu_1,...,\\mu_k $\n",
    "   Compute cost function(distortion)\n",
    "   $J(c^{(i)},...,c^{(m)},\\mu_1,...,\\mu_k)$\n",
    "}\n",
    "pick set of clusters that gave lowest cost J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# UNQ_C1\n",
    "# GRADED FUNCTION: find_closest_centroids\n",
    "\n",
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): (K, n) centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(X.shape[0]):\n",
    "        X_temp = np.tile(X[i], (3, 1))\n",
    "        norm_ij = np.linalg.norm(X_temp - centroids,axis=1) \n",
    "        idx[i] = np.argmin(norm_ij)\n",
    "        \n",
    "#     # Compute Euclidean distances between all data points and centroids\n",
    "#     distances = np.linalg.norm(centroids[:, np.newaxis, :] - X, axis=2)\n",
    "    \n",
    "#     # Find the index of the centroid with the minimum distance for each data point\n",
    "#     idx = np.argmin(distances, axis=0)  \n",
    "        \n",
    "     ### END CODE HERE ###\n",
    "    \n",
    "    return idx\n",
    "\n",
    "\n",
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_centroids\n",
    "\n",
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    centroids = np.zeros((K, n))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(K):\n",
    "        centroids[i] = (np.sum(X[idx==i],axis=0))/(X[idx==i].shape[0])\n",
    "        \n",
    "\n",
    "    # # Count the number of data points assigned to each centroid\n",
    "    # count_data_points = np.bincount(idx, minlength=K)\n",
    "    \n",
    "    # # Update the sums of data points assigned to each centroid\n",
    "    # np.add.at(sum_data_points, idx, X)\n",
    "    \n",
    "    # # Compute the new centroids by dividing the sums by the counts (avoiding division by zero)\n",
    "    # centroids = sum_data_points / count_data_points[:, np.newaxis]\n",
    "\n",
    "    ### END CODE HERE ## \n",
    "    \n",
    "    return centroids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
