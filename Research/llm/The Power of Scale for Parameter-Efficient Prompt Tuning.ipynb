{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Power of Scale for Parameter-Efficient Prompt Tuning\n",
    "\n",
    "\n",
    "## Comparison to Similar Approaches\n",
    "\n",
    "Among methods with learnable parameters, prompt tuning is the most parameter efficient, requiring less than 0.01% task-specific parameters for models over a billion parameters.\n",
    "\n",
    "Li and Liang (2021) propose “prefix tuning”: learning a sequence of prefixes that are prepended at every transformer layer. This is akin to learning transformer activations that are fixed across exam- ples at every network layer. In contrast, prompt tuning uses a single prompt representation that is prepended to the embedded input. Beyond re- quiring fewer parameters, our approach allows the transformer to update the intermediate-layer task representations, as contextualized by an input ex- ample. Their work builds on GPT-2 (Radford et al., 2019) and BART (Lewis et al., 2020), while ours fo- cuses on T5 and examines changes in performance and robustness to design choices as model size in- creases. When using BART, prefix tuning includes prefixes on both the encoder and decoder network, while prompt tuning only requires prompts on the encoder. Li and Liang (2021) also rely on a repa- rameterization of the prefix to stabilize learning, which adds a large number of parameters during training, whereas our configuration does not re- quire this reparameterization and is robust across SuperGLUE tasks and model sizes.\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "https://arxiv.org/pdf/2104.08691.pdf \n",
    "https://huggingface.co/docs/peft/index "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
