{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Background:**\n",
    "   - In 2007, a meeting discussed deep neural networks and their role in artificial intelligence (AI).\n",
    "   - Geoffrey Hinton, a key figure in deep neural networks, aimed to use AI to understand the human brain.\n",
    "\n",
    "- **Learning Algorithm in Deep Nets:**\n",
    "   - Deep neural networks, powered by backpropagation (backprop), have been successful in various AI tasks.\n",
    "   - Backprop allows networks to learn from data, enabling tasks like image classification, speech recognition, and more.\n",
    "\n",
    "- **Biological neurons:**\n",
    "   - The brain has around 10 billion neurons.\n",
    "   - Each neuron is connected to about 10,000 other neurons. \n",
    "   - Neurons receive electrochemical inputs from other neurons at dendrites. \n",
    "   - If the input sum is large enough, the neuron fires and transmits a signal down its axon to connected neurons.\n",
    "   - Neurons fire in an all-or-nothing manner based on whether the input exceeds a threshold. \n",
    "   - The brain performs complex computation using many simple processing units (neurons) that transmit binary signals.\n",
    "   - Artificial neural networks are inspired by this biological model but are far simpler, though useful for certain tasks like image recognition.\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/Figure-B1-Illustration-of-two-biological-neurons-cell-body-1-and-2-and-their-main.png.jpeg\" alt=\"Alt text\" width=\"400\" height=\"200\" />\n",
    "  <img src=\"images/fsge8.png\" alt=\"Alt text\" width=\"400\" height=\"200\" />\n",
    "</div>\n",
    "\n",
    "- von Neumann architecture used in traditional computers and the architecture of neural networks:\n",
    "   - The von Neumann architecture is based on executing sequential instructions stored in memory. This works well for problems with definite algorithms and rules.\n",
    "   - Neural networks are massively parallel, modeled after biological neural networks. Each neuron processes its inputs and produces an output simultaneously. \n",
    "   - Neural networks learn from labeled training data, adjusting connection strengths between neurons. They are good for problems where it's hard to specify an algorithm, like pattern recognition.\n",
    "   - The von Neumann bottleneck limits performance due to sequential data transfers between CPU and memory. Neural networks avoid this by performing computation locally in each neuron.\n",
    "   - Neural networks can continue functioning even if some nodes fail, exhibiting fault tolerance. Traditional architectures are less fault tolerant.\n",
    "   - Traditional computers are deterministic, producing the same outputs given the same inputs and instructions. Neural networks involve stochastic elements and may produce different results with the same input.\n",
    "So in summary, the massive parallelism, ability to learn from data, and fault tolerance of neural networks make them suitable for different types of problems than traditional von Neumann architectures, particularly problems involving ambiguity, imprecise data, and pattern recognition. Their computational principles are fundamentally different.\n",
    "\n",
    "- **Challenge for Biological Brains:**\n",
    "   - Real brains are unlikely to use backpropagation due to incompatibility with the brain's anatomy and physiology, especially in the cortex.\n",
    "\n",
    "- **Alternative Learning Mechanisms:**\n",
    "   - Researchers, inspired by Hinton, are exploring biologically plausible learning mechanisms.\n",
    "   - Promising alternatives include feedback alignment, equilibrium propagation, predictive coding, and incorporating properties of cortical neurons.\n",
    "\n",
    "- **Hebbian Learning Rule:**\n",
    "   - Traditional neuroscientific learning theories were guided by Donald Hebb's rule: \"Neurons that fire together, wire together.\"\n",
    "   - However, this rule had limitations, especially for large networks learning from mistakes.\n",
    "\n",
    "- **Backpropagation in AI:**\n",
    "   - Backpropagation was introduced in 1986, allowing effective training of artificial neural networks with hidden layers.\n",
    "   - It involves a forward phase (inference) and a backward phase (updating synaptic weights based on errors).\n",
    "\n",
    "- **Biological Implausibility of Backprop:**\n",
    "   - Backpropagation is considered biologically implausible for several reasons, including the weight transport problem and the brain's limited access to information.\n",
    "\n",
    "- **Biologically Plausible Variations:**\n",
    "   - Efforts to find biologically plausible variations of backpropagation include feedback alignment and equilibrium propagation.\n",
    "   - Feedback alignment, for example, uses random values for the backward pass, surprising researchers with its effectiveness.\n",
    "\n",
    "- **Predictive Coding:**\n",
    "   - Predictive coding, a new view of perception, aligns with backpropagation requirements in a biologically plausible way.\n",
    "   - It involves hierarchical layers of neural processing, with error signals flowing upward to minimize prediction errors.\n",
    "\n",
    "- **Pyramidal Neurons:**\n",
    "    - Models based on pyramidal neurons, found in the cortex, suggest they could perform both forward and backward computations simultaneously.\n",
    "    - Pyramidal neurons' structure allows them to handle the weight transport problem.\n",
    "\n",
    "- **Role of Attention:**\n",
    "    - Attention is proposed as a solution to the lack of a \"teacher\" in the brain for error calculation.\n",
    "    - Attentional feedback signals combined with global reinforcement signals may enable backpropagation-like learning.\n",
    "\n",
    "- **Optimism and Challenges:**\n",
    "    - Optimism exists among computational neuroscientists, with advancements in identifying learning rules.\n",
    "    - Challenges remain, and empirical evidence supporting these proposed mechanisms in living brains is still elusive.\n",
    "\n",
    "- \n",
    "   - A new theory called the \"information bottleneck\" is proposed to explain the success of deep learning in artificial intelligence (AI) algorithms.\n",
    "   - Deep neural networks, inspired by the structure of the human brain, learn by strengthening or weakening connections to improve signal transmission from input data to high-level concepts.\n",
    "   - The information bottleneck theory suggests that deep learning involves compressing noisy input data through a bottleneck, retaining only the most relevant features for general concepts.\n",
    "   - Naftali Tishby, a computer scientist and neuroscientist, presented evidence supporting the information bottleneck theory in a conference talk.\n",
    "   - The theory suggests that the most important part of learning is forgetting irrelevant details, enabling the formation of general concepts.\n",
    "   - Experiments with small neural networks showed that deep learning involves two phases: a fitting phase where the network learns to label training data and a compression phase where it becomes adept at generalization.\n",
    "   - During the compression phase, the network sheds information about input data, retaining only the most relevant features for accurate labeling.\n",
    "   - The information bottleneck theory may have implications for the future of deep neural network research, providing a theoretical tool for understanding and improving neural network performance.\n",
    "   - Some researchers are optimistic about the theory, while others remain skeptical, emphasizing the need for further investigation into its applicability to different deep learning scenarios.\n",
    "   - The theory may offer insights into both artificial and human learning, but researchers acknowledge that the brain's learning mechanisms are likely more complex than those observed in deep neural networks.\n",
    "   - The article highlights ongoing discussions within the AI community about the theoretical foundations of deep learning and its potential applications in various domains.\n",
    "\n",
    "\n",
    "- **Historical Evolution:** Neural networks were proposed in 1944 but faced a decline in interest in 1969, only to resurge in the 1980s. They fell out of favor again in the early 2000s but made a powerful comeback in the second decade of the 21st century, driven by increased graphics processing power.\n",
    "\n",
    "- **Perceptron:** The first trainable neural network, the Perceptron, was introduced in 1957 by Frank Rosenblatt. Despite early promise, neural network research faced setbacks in 1959 when limitations were identified, leading to a temporary decline in interest.\n",
    "\n",
    "- **Resurgence in the 1980s:** In the 1980s, researchers developed more efficient algorithms for modifying neural networks, leading to a resurgence in interest and research.\n",
    "\n",
    "- **Unsatisfying Aspects:** Despite their success, neural networks have intellectual challenges. The settings learned during training might not reveal the underlying logic of how the network classifies data, making them somewhat opaque.\n",
    "\n",
    "- **Support Vector Machines:** In the early 2000s, support vector machines briefly supplanted neural networks in popularity due to their clean and elegant mathematical foundation.\n",
    "\n",
    "- **GPU and Deep Learning Revolution:** The resurgence of neural networks in recent years, known as the deep learning revolution, is attributed to the development of graphics processing units (GPUs) in the computer game industry. Modern GPUs enabled the expansion of neural networks into deep architectures with numerous layers.\n",
    "\n",
    "- **Current State:** Deep learning, with its deep neural networks, dominates various artificial intelligence research areas, producing state-of-the-art systems.\n",
    "\n",
    "- **Theoretical Advancements:** Researchers are making progress in understanding the theoretical aspects of neural networks. Recent work addresses the range of computations, global optimization, and overfitting issues, aiming to enhance the understanding and reliability of neural networks.\n",
    "\n",
    "- **Future Outlook:** Theoretical advancements could contribute to breaking the historical cycle of neural networks falling in and out of favor over the past seven decades.\n",
    "\n",
    "- **Reference** \n",
    "   - https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Biology/index.html \\\n",
    "   - https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/#:~:text=Backpropagation%20is%20a%20method%20for,to%20improve%20the%20network%27s%20performance \\\n",
    "   - https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414 \\\n",
    "   - https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/ "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
