{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Activation Function\n",
    "type of activation function:\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- ReLU\n",
    "- Leaky ReLU\n",
    "- Softmax\n",
    "\n",
    "## Sigmoid\n",
    "The sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function shown in the first figure and defined by the formula\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "The sigmoid function is bounded by a horizontal asymptote as $x \\to \\pm \\infty$.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/sigmoid.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "The sigmoid function is the activation function of choice for models where we need to predict the probability as an output. It is a smooth gradient function which prevents “jumps” in output values. Therefore, it prevents drastic changes in the output for small changes in the input.\n",
    "\n",
    "loss for sigmoid function:\n",
    "\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability of the label being true.\n",
    "\n",
    "## Tanh\n",
    "\n",
    "The tanh function is a rescaled version of the logistic sigmoid, such that its outputs range from -1 to 1:\n",
    "\n",
    "$$\\tanh(x) = 2\\sigma(2x) - 1 = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/tanh.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "The tanh function is also sigmoidal (s - shaped). It’s actually mathematically shifted version of the sigmoid function. Both are similar, and both take a real-valued number and “squash” it into range between 0 and 1, but where the sigmoid function only squashes positive numbers, the tanh function squashes all real numbers.\n",
    "\n",
    "loss for tanh function:\n",
    "\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability of the label being true.\n",
    "\n",
    "## ReLU\n",
    "\n",
    "ReLU stands for Rectified Linear Unit. It takes a real-valued input and thresholds it at zero (replaces negative values with zero)\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/relu.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "ReLU is the most commonly used activation function in neural networks, especially in CNNs. When compared to a function like the sigmoid function, ReLU helps a neural network learn much faster and prevents the vanishing gradient problem.\n",
    "\n",
    "loss for ReLU function:\n",
    "\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability of the label being true.\n",
    "\n",
    "## Leaky ReLU\n",
    "\n",
    "Leaky ReLU is an attempt to solve the dying ReLU problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes\n",
    "\n",
    "$$f(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ 0.01x & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/leakyrelu.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "loss for Leaky ReLU function:\n",
    "\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability of the label being true.\n",
    "\n",
    "## Softmax\n",
    "\n",
    "The softmax function is a generalization of the logistic function and represents a probability distribution over K different possible outcomes.\n",
    "\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "\n",
    "where $j = 1, \\ldots, K$.\n",
    "\n",
    "When applied to the entire vector $\\mathbf{z}$, the softmax function generates a vector of probabilities $\\mathbf{a}(x)$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}(x) = \\left[ \\frac{e^{z_1}}{\\sum_{k=1}^{N}{e^{z_k}}}, \\frac{e^{z_2}}{\\sum_{k=1}^{N}{e^{z_k}}}, \\ldots, \\frac{e^{z_N}}{\\sum_{k=1}^{N}{e^{z_k}}} \\right]\n",
    "$$\n",
    "\n",
    "The output $\\mathbf{a}$ is a vector of length $N$, so for softmax regression, you could also write:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}(x) = \\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "= \\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/softmax.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "loss for softmax function:\n",
    "\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^M y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "where $y_{ij}$ is the true label and $\\hat{y}_{ij}$ is the predicted probability of the label being true.\n",
    "\n",
    "The softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear variant of multinomial logistic regression.\n",
    "It is used in multinomial logistic regression and is often used as the last layer of a neural network-based classifier.\n",
    "\n",
    "\n",
    "code for activation function plot:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "# softmax\n",
    "y = np.exp(x) / np.sum(np.exp(x))\n",
    "# sigmoid\n",
    "# y = 1 / (1 + np.exp(-x))\n",
    "# tanh\n",
    "# y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "# relu\n",
    "# y = np.maximum(0, x)\n",
    "# leaky relu\n",
    "# y = np.maximum(0.1 * x, x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('softmax function')\n",
    "# plt.title('softmax function')\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loss Function\n",
    "type of loss function:\n",
    "- Mean Squared Error\n",
    "- Cross Entropy\n",
    "- Hinge Loss or Multi class SVM Loss\n",
    "- Sparse Multiclass Cross Entropy\n",
    "\n",
    "## Mean Squared Error\n",
    "\n",
    "The mean squared error (MSE) is the average of the squared errors between the predicted values and the actual value. It is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate.\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted label.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/meansquareerror.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "## Cross Entropy\n",
    "\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)$$\n",
    "\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability of the label being true.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/crossentropy.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "## Hinge Loss or Multi class SVM Loss\n",
    "\n",
    "Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. It is the max(0,1-t*y) function where t*y is the output of the SVM model. The function returns 0 if the predicted output t*y is greater than or equal to one. Otherwise, it returns 1-t*y which is always a positive value. The loss is small when the predicted output is of the correct sign (positive or negative) and large when the predicted output is of the wrong sign.\n",
    "\n",
    "$$L = \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)$$\n",
    "\n",
    "where $s_j$ is the score for class $j$ and $s_{y_i}$ is the score for the correct class $y_i$.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/hingeloss.png\" alt=\"Alt text\" width=\"400\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "code for loss function plot:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-3, 5, 100)\n",
    "# mean squared error\n",
    "y = (x - 1) ** 2\n",
    "# cross entropy\n",
    "# y = -np.log(x)\n",
    "# hinge loss\n",
    "# y = np.maximum(0, 1 - x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('mean square function')\n",
    "plt.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
