{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "Definetion: A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
    "\n",
    "## RNN\n",
    "\n",
    "process :\n",
    "\n",
    "- take input $x_t$ at time step $t$, hidden state $h_{t-1}$ of the previous layer at time step $t-1$ , and for the first time step, the previous hidden-state $h_{t-1}$ is usually initialized to all zeros.\n",
    "- output $h_t$ at time step $t$\n",
    "- use $h_t$ as input at the next time step $t+1$\n",
    "- repeat this process for all inputs\n",
    "\n",
    "\n",
    "Equation of RNN:\n",
    "\n",
    "it is a simple neural network with a hidden layer. The input $x_t$ at time step $t$ is multiplied by a weight matrix $W_{xh}$ and added to the hidden state $h_{t-1}$ of the previous time step multiplied by the hidden-to-hidden weight matrix $W_{hh}$.giving an intermediate state $h_t$. The hidden state $h_t$ is then mapped to an output $y_t$ by multiplying it with the hidden-to-output weight matrix $W_{hy}$.\n",
    "\n",
    "$$ h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t) $$\n",
    "$$ y_t = W_{hy}h_t $$\n",
    "\n",
    "where \n",
    "- $h_t$ is the hidden state at time step $t$\n",
    "- $x_t$ is the input at time step $t$\n",
    "- $y_t$ is the output at time step $t$\n",
    "- $W_{hh}$ is the hidden-to-hidden weight matrix\n",
    "- $W_{xh}$ is the input-to-hidden weight matrix\n",
    "- $W_{hy}$ is the hidden-to-output weight matrix\n",
    "\n",
    "## LSTM\n",
    "\n",
    "LSTM is a special kind of RNN, capable of learning long-term dependencies. It was introduced by Hochreiter & Schmidhuber (1997), and was refined and popularized by many people in following work. It works tremendously well on a large variety of problems and is now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. \n",
    "\n",
    "You can observe how the tanh function transforms the candidate cell state to be centered and scaled between -1 and 1, which is essential for maintaining the balance of information within the LSTM cell. \n",
    "scaled_candidate_cell_state: We apply the tanh function to the candidate cell state, resulting in a scaled and centered version of the information. The tanh function squashes the values to be between -1 and 1, centering them around 0. This ensures that the information in the cell state is symmetric and bounded.\n",
    "\n",
    "tanh is to scale the values between -1 and 1, which is essential for maintaining the balance of information within the LSTM cell.\n",
    "sigmoid is to decide which information to throw away and which information to keep.\n",
    "\n",
    "gates \n",
    "forget gate - \n",
    "input gate -\n",
    "output gate -\n",
    "\n",
    "$H_t$ is the hidden state is short-term memory and $C_t$ is the long-term memory. \n",
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/lstm_architecture.png\" alt=\"Alt text\" width=\"400\" height=\"200\" />\n",
    "</div>\n",
    "\n",
    "$$ f_t = \\sigma(W_f[h_{t-1},x_t] + b_f) $$\n",
    "$$ i_t = \\sigma(W_i[h_{t-1},x_t] + b_i) $$\n",
    "$$ o_t = \\sigma(W_o[h_{t-1},x_t] + b_o) $$\n",
    "$$ g_t = \\tanh(W_g[h_{t-1},x_t] + b_g) $$\n",
    "$$ c_t = f_t * c_{t-1} + i_t * g_t $$\n",
    "$$ h_t = o_t * \\tanh(c_t) $$\n",
    "\n",
    "where\n",
    "- $f_t$ is forget gate\n",
    "- $i_t$ is input gate\n",
    "- $o_t$ is output gate\n",
    "- $g_t$ is gate gate\n",
    "- $c_t$ is cell state\n",
    "- $h_t$ is hidden state\n",
    "- $W_f$ is weight matrix of forget gate\n",
    "- $W_i$ is weight matrix of input gate\n",
    "- $W_o$ is weight matrix of output gate\n",
    "- $W_g$ is weight matrix of gate gate\n",
    "- $b_f$ is bias of forget gate\n",
    "- $b_i$ is bias of input gate\n",
    "- $b_o$ is bias of output gate\n",
    "- $b_g$ is bias of gate gate\n",
    "\n",
    "\n",
    "\n",
    "## GRU\n",
    "\n",
    "GRU is a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRU's have been shown to exhibit better performance on certain smaller datasets. \n",
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/gru_architecture.png\" alt=\"Alt text\" width=\"500\" height=\"400\" />\n",
    "</div>\n",
    "\n",
    "$$ z_t = \\sigma(W_z[h_{t-1},x_t]) $$\n",
    "$$ r_t = \\sigma(W_r[h_{t-1},x_t]) $$\n",
    "$$ \\tilde{h_t} = \\tanh(W[h_{t-1},r_t * x_t]) $$\n",
    "$$ h_t = (1-z_t) * h_{t-1} + z_t * \\tilde{h_t} $$\n",
    "\n",
    "where\n",
    "- $z_t$ is update gate\n",
    "- $r_t$ is reset gate\n",
    "- $\\tilde{h_t}$ is new gate\n",
    "- $h_t$ is hidden state\n",
    "- $W_z$ is weight matrix of update gate\n",
    "- $W_r$ is weight matrix of reset gate\n",
    "- $W$ is weight matrix of new gate\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### RNN\n",
    "\n",
    "```python\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "```\n",
    "\n",
    "### LSTM\n",
    "\n",
    "```python\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2g = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2y = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        forget = torch.sigmoid(self.i2f(combined))\n",
    "        input = torch.sigmoid(self.i2i(combined))\n",
    "        output = torch.sigmoid(self.i2o(combined))\n",
    "        gate = torch.tanh(self.i2g(combined))\n",
    "        cell = forget * cell + input * gate\n",
    "        hidden = output * torch.tanh(cell)\n",
    "        output = self.i2y(torch.cat((input, hidden), 1))\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def initCell(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "```\n",
    "\n",
    "### GRU\n",
    "\n",
    "```python\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2z = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2r = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2y = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        update = torch.sigmoid(self.i2z(combined))\n",
    "        reset = torch.sigmoid(self.i2r(combined))\n",
    "        new = torch.tanh(self.i2h(torch.cat((input, reset * hidden), 1)))\n",
    "        hidden = (1 - update) * hidden + update * new\n",
    "        output = self.i2y(torch.cat((input, hidden), 1))\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "```\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "- https://www.youtube.com/watch?v=YCzL96nL7j0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Gate Value: 0.574442516811659 Input Gate Value: 0.6681877721681662 Output Gate Value: 0.6224593312018546\n",
      "Candidate Cell State (Before tanh): [ 0.6  0.3 -0.2]\n",
      "Scaled Candidate Cell State (After tanh): [ 0.53704957  0.29131261 -0.19737532]\n",
      "Updated Cell State: [0.51580117 0.25790058 0.09613945]\n",
      "Hidden State: [0.2953276  0.15706568 0.05965921]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Simulate forget gate, input gate, and output gate values\n",
    "forget_gate_value = sigmoid(0.3)  # Example value between 0 and 1\n",
    "input_gate_value = sigmoid(0.7)  # Example value between 0 and 1\n",
    "output_gate_value = sigmoid(0.5)  # Example value between 0 and 1\n",
    "print(\"Forget Gate Value:\", forget_gate_value, \"Input Gate Value:\", input_gate_value, \"Output Gate Value:\", output_gate_value)\n",
    "# Simulate previous cell state and candidate cell state\n",
    "prev_cell_state = np.array([0.2, 0.1, 0.4])  # Example previous cell state\n",
    "candidate_cell_state = np.array([0.6, 0.3, -0.2])  # Example candidate cell state\n",
    "# Apply the tanh function to the candidate cell state\n",
    "scaled_candidate_cell_state = np.tanh(candidate_cell_state)\n",
    "\n",
    "print(\"Candidate Cell State (Before tanh):\", candidate_cell_state)\n",
    "print(\"Scaled Candidate Cell State (After tanh):\", scaled_candidate_cell_state)\n",
    "\n",
    "# Update the cell state using the gates\n",
    "updated_cell_state = forget_gate_value * prev_cell_state + input_gate_value * candidate_cell_state\n",
    "# Simulate how information is exposed in the hidden state\n",
    "hidden_state = output_gate_value * np.tanh(updated_cell_state)\n",
    "\n",
    "print(\"Updated Cell State:\", updated_cell_state)\n",
    "print(\"Hidden State:\", hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple 10 line rnn text = \"hello world\" give this text to the neural network and it will predict the next character\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data)) # set() returns unique characters\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size)) # data has 1115394 characters, 65 unique.\n",
    "\n",
    "# create two dictionaries to map characters to integers and integers to characters\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # character to index\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # index to character\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1 # learning rate for gradient descent\n",
    "\n",
    "# model parameters\n",
    "# weights\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "# biases\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "# loss function\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  # store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  # each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 25 arrays of size 100 (size of hidden layer)\n",
    "  # xs will store 25 inputs (each of size 65)\n",
    "  # hs will store 25 hidden states (each of size 100)\n",
    "  # ys will store 25 outputs (each of size 65)\n",
    "  # ps will store 25 probability values (each of size 65)\n",
    "  # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "  # so it's better to use dicts in this case\n",
    "  # init with previous hidden state\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  # print(\"hs[-1] = \", hs[-1].shape)\n",
    "  # print(\"hprev = \", hprev.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init loss as 0\n",
    "loss = 0\n",
    "# forward pass\n",
    "for t in range(len(inputs)):\n",
    "    # print(\"t = \", t)\n",
    "    # print(\"inputs[t] = \", inputs[t])\n",
    "    # print(\"Wxh = \", Wxh.shape)\n",
    "    # print(\"xs[t] = \", xs[t].shape)\n",
    "    # print(\"Whh = \", Whh.shape)\n",
    "    # print(\"hs[t-1] = \", hs[t-1].shape)\n",
    "    # print(\"bh = \", bh.shape)\n",
    "    # print(\"hs[t] = \", hs[t].shape)\n",
    "    # print(\"Why = \", Why.shape)\n",
    "    # print(\"ys[t] = \", ys[t].shape)\n",
    "    # print(\"by = \", by.shape)\n",
    "    # print(\"ps[t] = \", ps[t].shape)\n",
    "    # print(\"targets[t] = \", targets[t])\n",
    "    # print(\"ys[t][targets[t]] = \", ys[t][targets[t]])\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    # print(\"np.log(ps[t][targets[t]]) = \", np.log(ps[t][targets[t]]))\n",
    "    # print(\"loss = \", loss)\n",
    "    xs[t] = np.zeros((vocab_size, 1)) # encode in 1-of-k representation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# simple rnn model\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import mnist data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# define hyperparameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# network parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# define rnn model\n",
    "def RNN(x, weights, biases):\n",
    "    # unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    # define lstm cell\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    # get lstm cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    # linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "# construct model\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    \n",
    "    # keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        \n",
    "        # run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                 \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                 \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy: \", \\\n",
    "         sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n",
    "\n",
    "# visualize the result\n",
    "    test_pred = sess.run(pred, feed_dict={x: test_data})\n",
    "    test_pred = np.argmax(test_pred, axis=1)\n",
    "    test_label = np.argmax(test_label, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(15):\n",
    "        plt.subplot(3, 5, i+1)\n",
    "        plt.imshow(test_data[i].reshape((28, 28)), cmap='gray')\n",
    "        plt.title('label={}, pred={}'.format(test_label[i], test_pred[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# close the session\n",
    "sess.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple rnn explane with formula\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn with numpy implementation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# define derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# define hyperparameters\n",
    "learning_rate = 0.1\n",
    "training_iters = 100000\n",
    "display_step = 10000\n",
    "\n",
    "# network parameters\n",
    "n_input = 2\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "\n",
    "# initialize weights\n",
    "W_hidden = np.random.uniform(size=(n_input, n_hidden))\n",
    "b_hidden = np.random.uniform(size=(1, n_hidden))\n",
    "W_output = np.random.uniform(size=(n_hidden, n_output))\n",
    "b_output = np.random.uniform(size=(1, n_output))\n",
    "\n",
    "# training dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# training\n",
    "for i in range(training_iters):\n",
    "    # forward propagation\n",
    "    hidden_layer_input = np.dot(X, W_hidden) + b_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, W_output) + b_output\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # backpropagation\n",
    "    error = y - output_layer_output\n",
    "    d_output = error * sigmoid_derivative(output_layer_output)\n",
    "    error_hidden_layer = d_output.dot(W_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # update weights\n",
    "    W_output += hidden_layer_output.T.dot(d_output) * learning_rate\n",
    "    b_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    W_hidden += X.T.dot(d_hidden_layer) * learning_rate\n",
    "    b_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    # print loss\n",
    "    if i % display_step == 0:\n",
    "        print('Iter {}, Loss: {:.6f}'.format(i, np.mean(np.abs(error))))\n",
    "\n",
    "# testing\n",
    "hidden_layer_input = np.dot(X, W_hidden) + b_hidden\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_output, W_output) + b_output\n",
    "output_layer_output = sigmoid(output_layer_input)\n",
    "print('Output: ', output_layer_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
