{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Decision trees are a type of supervised learning algorithm that can be used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "## Decision tree Steps\n",
    "\n",
    "1. Calculate the entropy of the target.\n",
    "2. Calculate the entropy of the target for each feature.\n",
    "3. Calculate the information gain for each feature.\n",
    "4. Choose the feature with the largest information gain as the root node.\n",
    "5. Repeat steps 1 to 4 for each branch until you get the desired tree depth.\n",
    "\n",
    "## Decision tree for classification\n",
    "\n",
    "Entropy is the measure of impurity in a bunch of examples. The entropy of a set $S$ is defined as:\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where $p_i$ is the proportion of the ith class.\n",
    "\n",
    "The entropy is 0 if all samples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution. For example, in a binary class setting, the entropy is 0 if $p_1 = 1$ or $p_2 = 0$. If the classes are distributed uniformly with $p_1 = p_2 = 0.5$, the entropy is 1. Therefore, we can say that the entropy reaches its maximum value if the classes are uniformly distributed.\n",
    "\n",
    "The following equation shows how to calculate the entropy of a dataset $D$:\n",
    "\n",
    "$$\n",
    "H(D) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where \n",
    "- $p_i$ is the proportion of the ith class\n",
    "- $c$ is the number of classes\n",
    "- $y$ is the class label.\n",
    "\n",
    "For $y = 0$ and $y = 1$ (binary class setting), we can rewrite the equation as follows:\n",
    "\n",
    "$$\n",
    "H(D_1) = -p_1 \\log_2(p_1) - (1 - p_1) \\log_2(1 - p_1)\n",
    "$$\n",
    "\n",
    "where \n",
    "- $ p_1 $ is the proportion of the positive class\n",
    "- $p_2 = 1 - p_1$ is the proportion of the negative class\n",
    "- $D_1$ is the dataset of the left node.\n",
    "\n",
    "The information gain is the entropy of the dataset before the split minus the weighted entropy after the split by an attribute. The following equation shows how to calculate the information gain $IG$ for a decision tree:\n",
    "\n",
    "$$\n",
    "IG(D_p, f) = H(D_p) - \\sum_{j=1}^{m} \\frac{N_j}{N_p} H(D_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "IG(D_p, f) = H(D_p) - \\sum_{j=1}^{m} \\frac{N_j}{N_p} \\left(-p_{j1} \\log_2(p_{j1}) - p_{j2} \\log_2(p_{j2})\\right)\n",
    "$$\n",
    "where \n",
    "- $f$ is the feature to perform the split\n",
    "- $D_p$ and $D_j$ are the dataset of the parent and $j$th child node\n",
    "- $N_p$ is the total number of samples at the parent node\n",
    "- $N_j$ is the number of samples in the $j$th child node\n",
    "- $m$ is the number of child nodes\n",
    "\n",
    "For $y = 0$ and $y = 1$ and $m = 2$(binary class setting) and two child nodes, we can rewrite the equation as follows:\n",
    "$$ IG(D_p, f) = H(D_p) - \\sum_{j=1}^{2} \\frac{N_j}{N_p} H(D_j) $$\n",
    "$$ IG(D_p, f) = H(D_p) - (\\frac{N_{left}}{N_p} H(D_{left}) + \\frac{N_{right}}{N_p} H(D_{right})) $$\n",
    "$$ IG(D_p, f) = H(D_p) - (\\frac{N_{left}}{N_p} \\left(-p_{left1} \\log_2(p_{left1}) - (1 - p_{left1}) \\log_2(1 - p_{left1})\\right) + \\frac{N_{right}}{N_p} \\left(-p_{right1} \\log_2(p_{right1}) - (1 - p_{right1}) \\log_2(1 - p_{right1})\\right)) $$\n",
    "\n",
    "where\n",
    "where\n",
    "- $p_{j1}$ is the proportion of the positive class in the $j$th child node\n",
    "- $p_{j2} = 1 - p_{j1}$ is the proportion of the negative class in the $j$th child node\n",
    "\n",
    "## Gini impurity \n",
    "Gini impurity is another criterion that is often used in training decision trees:\n",
    "\n",
    "$$Gini(p) = \\sum_{k=1}^{|\\mathcal{Y}|} p_{k} (1 - p_{k}) = \\sum_{k=1}^{|\\mathcal{Y}|} p_{k} - \\sum_{k=1}^{|\\mathcal{Y}|} p_{k}^2 = 1 - \\sum_{k=1}^{|\\mathcal{Y}|} p_{k}^2$$\n",
    "\n",
    "where $p_{k}$ is the proportion of the $k$th class.\n",
    "\n",
    "Imformation gain for the Gini impurity is calculated as follows:\n",
    "\n",
    "$$IG(D_p, f) = Gini(D_p) - \\sum_{j=1}^{m} \\frac{N_j}{N_p} Gini(D_j)$$\n",
    "\n",
    "where $f$ is the feature to perform the split, $D_p$ and $D_j$ are the dataset of the parent and $j$th child node, $N_p$ is the total number of samples at the parent node, and $N_j$ is the number of samples in the $j$th child node.\n",
    "\n",
    "## Classification error\n",
    "The classification error is another criterion that is often used in training decision trees:\n",
    "\n",
    "$$E = 1 - \\max_k p_{k}$$\n",
    "\n",
    "where $p_{k}$ is the proportion of the $k$th class.\n",
    "\n",
    "The information gain ratio is another criterion that is often used in training decision trees:\n",
    "\n",
    "$$IGR(D_p, f) = \\frac{IG(D_p, f)}{H(D_p)}$$\n",
    "\n",
    "where $f$ is the feature to perform the split, $D_p$ and $D_j$ are the dataset of the parent and $j$th child node, $N_p$ is the total number of samples at the parent node, and $N_j$ is the number of samples in the $j$th child node.\n",
    "\n",
    "The following code implements the entropy and information gain equations:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (10, 3)\n",
      "y_train shape:  (10,)\n",
      "Total number of samples:  10\n",
      "Number of features:  3\n",
      "\n",
      "entropy of root node 1.0\n",
      "\n",
      "CASE 1:\n",
      "Left indices:  [5, 6, 8]\n",
      "Right indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "CASE 2:\n",
      "Left indices:  [1, 2, 3, 6, 7, 9]\n",
      "Right indices:  [0, 4, 5, 8]\n",
      "\n",
      "information gain 0 :  0.034851554559677145\n",
      "information gain 1 :  0.12451124978365319\n",
      "information gain 2 :  0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a set of labels\n",
    "    \"\"\"\n",
    "    entropy = 0.\n",
    "    if len(y)==0:\n",
    "        return 0\n",
    "    p1 = np.sum(y == 1)/y.shape[0]\n",
    "    if p1==0 or p1==1:\n",
    "        return 0\n",
    "    entropy = -p1 * (np.log2(p1)) - (1 - p1) * (np.log2(1 - p1))\n",
    "    return entropy\n",
    " \n",
    "def split_dataset(X, node_indices, feature, threshold):\n",
    "    \"\"\"\n",
    "    Split dataset X into two subsets given a feature and a threshold\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i in node_indices:\n",
    "        if X[i, feature] <= threshold:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices\n",
    "\n",
    "def information_gain(X_train, y_train,root_indices, feature, threshold=0):\n",
    "    \"\"\"\n",
    "    Compute the information gain\n",
    "    \"\"\"\n",
    "    left_indices, right_indices = split_dataset(X_train, root_indices, feature, threshold)\n",
    "    left_entropy = compute_entropy(y_train[left_indices])\n",
    "    right_entropy = compute_entropy(y_train[right_indices])\n",
    "    entropy = compute_entropy(y_train)\n",
    "    if len(left_indices)==0 or len(right_indices)==0:\n",
    "        return 0\n",
    "    ig = entropy - (len(left_indices)/len(root_indices))*left_entropy - (len(right_indices)/len(root_indices))*right_entropy\n",
    "    return ig\n",
    "        \n",
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])\n",
    "# dataset shape \n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"Total number of samples: \", X_train.shape[0])\n",
    "print(\"Number of features: \", X_train.shape[1])\n",
    "# entropy of root node \n",
    "print(\"\\nentropy of root node\" ,compute_entropy(y_train))\n",
    "\n",
    "# split dataset into two subsets\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# split dataset into two subsets given a feature \n",
    "feature = 0\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature, threshold=0)\n",
    "print(\"\\nCASE 1:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# split dataset into two subsets given a feature\n",
    "feature = 1\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature, threshold=0)\n",
    "print(\"CASE 2:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# split dataset into two subsets given a feature\n",
    "print(\"\\ninformation gain 0 : \", information_gain(X_train, y_train, root_indices, feature=0, threshold=0))\n",
    "print(\"information gain 1 : \", information_gain(X_train, y_train, root_indices, feature=1, threshold=0))\n",
    "print(\"information gain 2 : \", information_gain(X_train, y_train, root_indices, feature=2, threshold=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "Best feature:  2 Best threshold:  0\n",
      "CASE 2:\n",
      "Best feature:  1 Best threshold:  0\n"
     ]
    }
   ],
   "source": [
    "# find the best split\n",
    "def get_best_split(X_train, y_train, root_indices):\n",
    "    \"\"\"\n",
    "    Find the best split for a node\n",
    "    \"\"\"\n",
    "    best_feature = 0\n",
    "    best_threshold = 0\n",
    "    max_ig = 0\n",
    "    for feature in range(X_train.shape[1]):\n",
    "        for threshold in range(0, 1):\n",
    "            ig = information_gain(X_train,y_train, root_indices, feature, threshold)\n",
    "            if ig > max_ig:\n",
    "                max_ig = ig\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    return best_feature, best_threshold \n",
    " \n",
    "# find the best split for a node\n",
    "print(\"CASE 1:\")\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "best_feature, best_threshold = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature: \", best_feature, \"Best threshold: \", best_threshold)\n",
    "print(\"CASE 2:\")\n",
    "root_indices = [0, 2, 4, 5, 6, 8]\n",
    "best_feature, best_threshold = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature: \", best_feature, \"Best threshold: \", best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_depth:  0 best_feature:  2 best_threshold:  0\n",
      "- current_depth:  1 best_feature:  1 best_threshold:  0\n",
      "-- current_depth:  2 best_feature:  0 best_threshold:  0\n",
      "   --- left leaf node with indices [6]\n",
      "   --- right leaf node with indices [2, 3, 9]\n",
      "-- current_depth:  2 best_feature:  0 best_threshold:  0\n",
      "   --- left leaf node with indices [8]\n",
      "- current_depth:  1 best_feature:  0 best_threshold:  0\n",
      "-- current_depth:  2 best_feature:  0 best_threshold:  0\n",
      "   --- left leaf node with indices [5]\n",
      "-- current_depth:  2 best_feature:  1 best_threshold:  0\n",
      "   --- left leaf node with indices [1, 7]\n",
      "   --- right leaf node with indices [0, 4]\n",
      "\n",
      "Tree with indices:\n",
      " Node with indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " - Node with indices: [2, 3, 6, 8, 9]\n",
      "  -- Node with indices: [2, 3, 6, 9]\n",
      "   --- Node with indices: [6]\n",
      "   --- Node with indices: [2, 3, 9]\n",
      "  -- Node with indices: [8]\n",
      "   --- Node with indices: [8]\n",
      " - Node with indices: [0, 1, 4, 5, 7]\n",
      "  -- Node with indices: [5]\n",
      "   --- Node with indices: [5]\n",
      "  -- Node with indices: [0, 1, 4, 7]\n",
      "   --- Node with indices: [1, 7]\n",
      "   --- Node with indices: [0, 4]\n"
     ]
    }
   ],
   "source": [
    "# build a decision tree recursively\n",
    "def build_tree_recursive(X_train, y_train, node_indices,current_depth, max_depth,branch_label=\"\"):\n",
    "    \"\"\"\n",
    "    Build a decision tree recursively\n",
    "    \"\"\"\n",
    "    # 1. compute information gain for all features\n",
    "    # 2. find the best feature and the best threshold\n",
    "    # 3. split dataset into two subsets\n",
    "    # 4. build left and right subtrees recursively\n",
    "    # 5. return a tree node\n",
    "\n",
    "    if len(node_indices)==0:\n",
    "        return None\n",
    "    \n",
    "    if max_depth==0:\n",
    "        return None\n",
    "    \n",
    "    if current_depth==max_depth:\n",
    "        formatting = \" \" * current_depth+\"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_label, node_indices)\n",
    "        return {'indices': node_indices}\n",
    "\n",
    "    best_feature, best_threshold = get_best_split(X_train, y_train, node_indices)\n",
    "    formatting = \"-\" * current_depth\n",
    "    print(formatting, \"current_depth: \", current_depth, \"best_feature: \", best_feature, \"best_threshold: \", best_threshold)\n",
    "    left_indices, right_indices = split_dataset(X_train, node_indices, best_feature, best_threshold)\n",
    "    node = {}\n",
    "    node['feature'] = best_feature\n",
    "    node['threshold'] = best_threshold\n",
    "    node['indices'] = node_indices\n",
    "    node['left'] = build_tree_recursive(X_train, y_train, left_indices, current_depth+1, max_depth, branch_label=\"left\")\n",
    "    node['right'] = build_tree_recursive(X_train, y_train, right_indices, current_depth+1, max_depth, branch_label=\"right\")\n",
    "    return node\n",
    "\n",
    "def print_tree_indices(tree, depth=0):\n",
    "    if tree is None:\n",
    "        return\n",
    "    formatting = \" \" * depth + \"-\" * depth\n",
    "    if 'indices' in tree:\n",
    "        print(formatting, \"Node with indices:\", tree['indices'])\n",
    "    \n",
    "    if 'left' in tree:\n",
    "        print_tree_indices(tree['left'], depth + 1)\n",
    "    \n",
    "    if 'right' in tree:\n",
    "        print_tree_indices(tree['right'], depth + 1)\n",
    "\n",
    "\n",
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])\n",
    "\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "node = build_tree_recursive(X_train, y_train, root_indices, current_depth=0, max_depth=3)\n",
    "print(\"\\nTree with indices:\")\n",
    "# You can call the function to print the tree with indices\n",
    "print_tree_indices(node)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini impurity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_depth:  0 best_feature:  2 best_threshold:  0\n",
      "- current_depth:  1 best_feature:  1 best_threshold:  0\n",
      "-- current_depth:  2 best_feature:  0 best_threshold:  0\n",
      "   --- left leaf node with indices [6]\n",
      "   --- right leaf node with indices [2, 3, 9]\n",
      "-- current_depth:  2 best_feature:  0 best_threshold:  0\n",
      "   --- left leaf node with indices [8]\n",
      "- current_depth:  1 best_feature:  0 best_threshold:  0\n",
      "-- current_depth:  2 best_feature:  0 best_threshold:  0\n",
      "   --- left leaf node with indices [5]\n",
      "-- current_depth:  2 best_feature:  1 best_threshold:  0\n",
      "   --- left leaf node with indices [1, 7]\n",
      "   --- right leaf node with indices [0, 4]\n",
      "\n",
      "Tree with indices:\n",
      " Node with indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " - Node with indices: [2, 3, 6, 8, 9]\n",
      "  -- Node with indices: [2, 3, 6, 9]\n",
      "   --- Node with indices: [6]\n",
      "   --- Node with indices: [2, 3, 9]\n",
      "  -- Node with indices: [8]\n",
      "   --- Node with indices: [8]\n",
      " - Node with indices: [0, 1, 4, 5, 7]\n",
      "  -- Node with indices: [5]\n",
      "   --- Node with indices: [5]\n",
      "  -- Node with indices: [0, 1, 4, 7]\n",
      "   --- Node with indices: [1, 7]\n",
      "   --- Node with indices: [0, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with gini index\n",
    "\n",
    "def compute_gini(y):\n",
    "    gini = 0.\n",
    "    if len(y)==0:\n",
    "        return 0\n",
    "    p1 = np.sum(y == 1)/y.shape[0]\n",
    "    if p1==0 or p1==1:\n",
    "        return 0\n",
    "    gini = 1 - p1**2 - (1 - p1)**2\n",
    "    return gini\n",
    "\n",
    "def gini_gain(X_train, y_train,root_indices, feature, threshold=0):\n",
    "    left_indices, right_indices = split_dataset(X_train, root_indices, feature, threshold)\n",
    "    left_gini = compute_gini(y_train[left_indices])\n",
    "    right_gini = compute_gini(y_train[right_indices])\n",
    "    gini = compute_gini(y_train)\n",
    "    if len(left_indices)==0 or len(right_indices)==0:\n",
    "        return 0\n",
    "    gg = gini - (len(left_indices)/len(root_indices))*left_gini - (len(right_indices)/len(root_indices))*right_gini\n",
    "    return gg\n",
    "\n",
    "def get_best_split_gini(X_train, y_train, root_indices):\n",
    "    best_feature = 0\n",
    "    best_threshold = 0\n",
    "    max_gg = 0\n",
    "    for feature in range(X_train.shape[1]):\n",
    "        for threshold in range(0, 1):\n",
    "            gg = gini_gain(X_train,y_train, root_indices, feature, threshold)\n",
    "            if gg > max_gg:\n",
    "                max_gg = gg\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "def build_tree_recursive_gini(X_train, y_train, node_indices,current_depth, max_depth,branch_label=\"\"):\n",
    "\n",
    "    if len(node_indices)==0:\n",
    "        return None\n",
    "    \n",
    "    if max_depth==0:\n",
    "        return None\n",
    "    \n",
    "    if current_depth==max_depth:\n",
    "        formatting = \" \" * current_depth+\"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_label, node_indices)\n",
    "        return {'indices': node_indices}\n",
    "\n",
    "    best_feature, best_threshold = get_best_split_gini(X_train, y_train, node_indices)\n",
    "    formatting = \"-\" * current_depth\n",
    "    print(formatting, \"current_depth: \", current_depth, \"best_feature: \", best_feature, \"best_threshold: \", best_threshold)\n",
    "    left_indices, right_indices = split_dataset(X_train, node_indices, best_feature, best_threshold)\n",
    "    node = {}\n",
    "    node['feature'] = best_feature\n",
    "    node['threshold'] = best_threshold\n",
    "    node['indices'] = node_indices\n",
    "    node['left'] = build_tree_recursive_gini(X_train, y_train, left_indices, current_depth+1, max_depth, branch_label=\"left\")\n",
    "    node['right'] = build_tree_recursive_gini(X_train, y_train, right_indices, current_depth+1, max_depth, branch_label=\"right\")\n",
    "    return node\n",
    "\n",
    "# X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "# y_train = np.array([1,1,0,0,1,0,0,1,1,0])\n",
    "\n",
    "# root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "node = build_tree_recursive_gini(X_train, y_train, root_indices, current_depth=0, max_depth=3)\n",
    "print(\"\\nTree with indices:\")\n",
    "# You can call the function to print the tree with indices\n",
    "print_tree_indices(node)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Ensemble:\n",
    "\n",
    "Tree ensembles are a powerful machine learning technique that addresses the limitations of single decision trees. They consist of multiple decision trees, each trained on slightly different versions of the data, and their predictions are combined to improve accuracy and robustness.\n",
    "\n",
    "- Weakness of Single Decision Trees: Single decision trees can be highly sensitive to minor data variations, leading to different predictions. This sensitivity limits their reliability.\n",
    "\n",
    "- Tree Ensembles for Robustness: Tree ensembles overcome sensitivity issues by using multiple decision trees. Each tree is trained on a different version of the data, creating diversity in the ensemble.\n",
    "\n",
    "- Voting in Tree Ensembles: When making predictions, all trees in the ensemble contribute their votes. The final prediction is determined by majority voting among the trees.\n",
    "\n",
    "- Robustness through Voting: Ensembles reduce sensitivity to individual tree behavior. Errors or variations in one tree's prediction have limited impact as each tree gets one vote.\n",
    "\n",
    "- Sampling with Replacement: To create diversity, tree ensembles use \"sampling with replacement.\" Randomly selecting examples from the original dataset with duplicates allows for various training subsets.\n",
    "\n",
    "- Constructing Random Training Sets: Multiple random training sets are generated by repeatedly selecting examples with replacement. Each tree is trained using one of these random sets.\n",
    "\n",
    "- Diversity in Ensemble: Randomness in training data and tree construction leads to a diverse collection of decision trees within the ensemble, enhancing accuracy.\n",
    "\n",
    "**Three Ensemble Techniques:**\n",
    "\n",
    "- **Bagging (Bootstrap Aggregation):** Bagging involves resampling the training set multiple times, training decision trees on each resampled set, and then aggregating their predictions. It improves stability and reduces overfitting.\n",
    "\n",
    "- **Boosting:** Boosting trains weak learners sequentially, with each learner focusing on examples previously misclassified by the ensemble. It adapts to errors and progressively enhances the model's accuracy.\n",
    "\n",
    "- **Stacking:** Stacking combines multiple models, typically using a meta-classifier or meta-regressor. Base-level models are trained independently, and their predictions serve as features for a meta-model, leading to improved predictive performance.\n",
    "\n",
    "<!-- ![tree_ensemble](images/tree_ensemble.png) -->\n",
    "\n",
    "--- \n",
    "\n",
    "## Bagging\n",
    "\n",
    "Bagging is an abbreviation for bootstrap aggregation. Bagging is a method that involves manipulating the training set by resampling, and then aggregating the predictions from each resampled training set. The following figure shows the bagging process:\n",
    "\n",
    "<!-- ![bagging](images/bagging.png) -->\n",
    "\n",
    "### Random forest\n",
    "\n",
    "Random forest is an ensemble of decision trees, where each tree is slightly different from the others. The idea behind random forest is to average multiple (deep) decision trees that individually suffer from high variance(overfitting), to build a more robust model that has a better generalization performance and is less susceptible to overfitting. The following figure shows a random forest with three decision trees:\n",
    "\n",
    "Building the Random Forest:\n",
    "\n",
    "- Multiple Trees: A Random Forest consists of a collection (ensemble) of decision trees. The number of trees is a hyperparameter, typically denoted as \"B.\"\n",
    "- Bootstrapping: To create each decision tree in the forest, a random subset of the original dataset is sampled with replacement. This process is called bootstrapping. As a result, each tree is trained on a slightly different subset of the data.\n",
    "- Feature Subsampling: At each node of a decision tree, instead of considering all features to determine the best split, a random subset of features (typically denoted as \"K\") is considered. This introduces an additional layer of randomness and helps ensure that different trees focus on different subsets of features.\n",
    "\n",
    "Training the Decision Trees:\n",
    "\n",
    "- Each decision tree in the Random Forest is trained independently using its bootstrapped subset of data and feature subsampling.\n",
    "- The decision trees aim to learn patterns and relationships in the data, making splits based on features and their values.\n",
    "- Splits are determined using criteria like information gain (for classification) or mean squared error reduction (for regression).\n",
    "\n",
    "Making Predictions:\n",
    "\n",
    "- For classification tasks, when a new data point needs to be classified, all the decision trees in the forest make predictions.\n",
    "- Each tree \"votes\" for a class label, and the class with the majority of votes becomes the final prediction.\n",
    "- For regression tasks, the predictions of all trees are averaged to produce the final prediction.\n",
    "\n",
    "Advantages of Random Forest:\n",
    "\n",
    "- Reduced Overfitting: By using multiple trees trained on different subsets of data, Random Forests are less prone to overfitting compared to a single decision tree.\n",
    "- High Accuracy: The ensemble nature of Random Forests typically results in higher accuracy and better generalization to new data.\n",
    "- Robustness: Random Forests are robust to noisy data and outliers due to the ensemble averaging.\n",
    "- Feature Importance: Random Forests can provide insights into feature importance, helping to identify which features are most influential in making predictions.\n",
    "\n",
    "Hyperparameters: \n",
    "- Random Forests have hyperparameters like the number of trees (B), the number of features to consider at each split (K), and tree-specific hyperparameters (e.g., tree depth). These can be tuned to optimize performance for specific tasks.\n",
    "\n",
    "<!-- ![random_forest](images/random_forest.png) -->\n",
    "\n",
    "--- \n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble machine learning method that combines multiple weak learners to create a strong learner. It focuses on training weak learners sequentially, with each learner attempting to correct the mistakes made by its predecessors. Here's a summary of the key points:\n",
    "\n",
    "- Ensemble Learning:\n",
    "   - Boosting is a type of ensemble learning, where multiple machine learning models, often referred to as \"weak learners,\" are combined to improve predictive accuracy.\n",
    "\n",
    "- Sequential Training:\n",
    "   - Boosting trains the weak learners sequentially, one after another.\n",
    "   - The training process emphasizes examples that the previous learners have classified incorrectly, allowing subsequent learners to focus on the challenging cases.\n",
    "\n",
    "- Weighted Examples:\n",
    "   - Examples in the training dataset are assigned weights, with misclassified examples given higher weights.\n",
    "   - This weighting mechanism ensures that the next weak learner gives more attention to the examples that the ensemble is struggling to classify correctly.\n",
    "\n",
    "- Adaptive Learning:\n",
    "   - As boosting progresses, each weak learner adapts to the errors made by the previous learners.\n",
    "   - The goal is to iteratively improve the ensemble's performance by reducing the bias and variance of the model.\n",
    "\n",
    "- Aggregating Predictions:\n",
    "   - After training all weak learners, boosting combines their predictions to make the final prediction.\n",
    "   - For classification tasks, a weighted majority vote is often used, where the weight of each learner's prediction depends on its accuracy.\n",
    "   - For regression tasks, predictions are typically aggregated by averaging.\n",
    "\n",
    "- Boosting Algorithms:\n",
    "   - Several boosting algorithms exist, with AdaBoost (Adaptive Boosting) and Gradient Boosting being two of the most well-known.\n",
    "   - These algorithms differ in how they assign weights to examples and update the model.\n",
    "\n",
    "- Strength in Weakness:\n",
    "   - The term \"weak learner\" refers to models that perform slightly better than random chance.\n",
    "   - Boosting demonstrates that by combining many weak learners, a strong learner with high predictive accuracy can be achieved.\n",
    "\n",
    "- Model Generalization:\n",
    "   - Boosting often leads to models with excellent generalization capabilities, making them suitable for a wide range of applications.\n",
    "\n",
    "<!-- ![boosting](images/boosting.png) -->\n",
    "\n",
    "**AdaBoost**\n",
    "\n",
    "AdaBoost is a boosting algorithm that combines multiple weak classifiers to build a strong classifier. AdaBoost assigns weights to individual training samples, and trains a weak classifier on the weighted training samples. The following figure shows the AdaBoost process:\n",
    "\n",
    "<!-- ![adaboost](images/adaboost.png) -->\n",
    "\n",
    "**Gradient boosting**\n",
    "\n",
    "Gradient boosting is a boosting algorithm that builds a strong classifier by combining multiple weak classifiers. Gradient boosting builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. The following figure shows the gradient boosting process:\n",
    "\n",
    "<!-- ![gradient_bo osting](images/gradient_boosting.png) -->\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "XGBoost stands for \"extreme gradient boosting\" and is a widely used open-source implementation of the boosted decision tree algorithm.It is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. The following figure shows the XGBoost process:\n",
    "\n",
    "**Motivation for Boosting:** Boosting is introduced as a technique to improve the performance of decision tree ensembles by focusing on examples that the ensemble of trees is not yet classifying accurately. It is likened to the concept of \"deliberate practice\" in learning.\n",
    "\n",
    "**Boosting Procedure:** \n",
    "   - After building each decision tree in the ensemble, the predictions of the ensemble are evaluated on the original training set.\n",
    "   - Examples that were misclassified receive higher attention during the next round of training to correct errors.\n",
    "   - This process continues for a total of \"B\" iterations, where each iteration focuses on improving predictions on examples previously misclassified by the ensemble.\n",
    "\n",
    "**XGBoost Advantages:**\n",
    "   - XGBoost is highlighted as a highly efficient and competitive implementation of the boosting algorithm.\n",
    "   - It offers default settings for splitting criteria and stopping criteria for tree construction.\n",
    "   - XGBoost includes built-in regularization techniques to prevent overfitting.\n",
    "   \n",
    "**XGBoost Working :** \n",
    "\n",
    "- XGBoost assigns different weights to training examples as a core part of its boosting algorithm. This approach helps to improve efficiency and focuses on challenging examples without the need to generate multiple randomly chosen training sets with replacement. Here's how this weighting process works:\n",
    "1. **Initial Weights:**\n",
    "   - At the beginning of the boosting process, all training examples are assigned equal weights, indicating their importance.\n",
    "\n",
    "2. **First Iteration:**\n",
    "   - In the first iteration, a decision tree (or weak learner) is trained on the initial weighted dataset.\n",
    "   - After training, the model makes predictions on the training data, and some examples may be misclassified while others are correctly classified.\n",
    "\n",
    "3. **Weight Adjustment:**\n",
    "   - XGBoost assigns higher weights to the misclassified examples and lower weights to the correctly classified ones.\n",
    "   - The intuition is that the model should pay more attention to examples it struggled with in the previous iteration.\n",
    "\n",
    "4. **Subsequent Iterations:**\n",
    "   - For each subsequent iteration (up to a specified number of boosting rounds), XGBoost repeats the process:\n",
    "     - It trains a new decision tree on the dataset with updated example weights.\n",
    "     - The weights are adjusted based on the performance of the model in the previous iteration.\n",
    "     - Examples that were misclassified in previous iterations receive higher weights, making them more influential in training the next tree.\n",
    "     - The model continues to refine its predictions by focusing on challenging examples.\n",
    "\n",
    "5. **Ensemble of Trees:**\n",
    "   - Over multiple boosting rounds, XGBoost builds an ensemble of decision trees, with each tree trained on a dataset where the weights reflect the difficulty of classifying each example.\n",
    "   - The final prediction is a weighted combination of the predictions from all trees in the ensemble.\n",
    "\n",
    "6. **Efficiency and Focus:**\n",
    "   - XGBoost's approach avoids the need to create multiple training datasets with replacement, which can be computationally expensive.\n",
    "   - Instead, it adapts the weights of examples directly, which makes the algorithm more efficient.\n",
    "   - By assigning higher weights to challenging examples, XGBoost ensures that the model devotes more effort to improving its performance on those cases.\n",
    "\n",
    "7. **Regularization:** In addition to focusing on challenging examples, XGBoost's weighting process also acts as a form of regularization, preventing the model from overfitting to the training data.\n",
    "\n",
    "8. **Practical Benefits:** This weighting mechanism contributes to XGBoost's effectiveness in competitions and real-world applications, as it helps the algorithm concentrate on improving predictions where they matter most.\n",
    "\n",
    "<!-- ![xgboost](images/xgboost.png) -->\n",
    "\n",
    "**LightGBM**\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "- Faster training speed and higher efficiency\n",
    "- Lower memory usage\n",
    "- Better accuracy\n",
    "- Support of parallel and GPU learning\n",
    "- Capable of handling large-scale data\n",
    "\n",
    "The following figure shows the LightGBM process:\n",
    "\n",
    "<!-- ![lightgbm](images/lightgbm.png) -->\n",
    "\n",
    "**CatBoost**\n",
    "\n",
    "CatBoost is a gradient boosting library that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "The following figure shows the CatBoost process:\n",
    "\n",
    "<!-- ![catboost](images/catboost.png) -->\n",
    "\n",
    "---\n",
    "\n",
    "# Stacking:\n",
    "\n",
    "Stacking is an ensemble learning technique that combines multiple classification or regression models using a meta-classifier (for classification) or a meta-regressor (for regression). It involves two levels of model training, where base-level models are trained on the complete training set, and a meta-model is trained on the outputs of the base-level models as features. Here's a summary of the key points:\n",
    "\n",
    "1. **Ensemble Learning:**\n",
    "   - Stacking is a form of ensemble learning, where the goal is to improve predictive accuracy by combining the strengths of multiple models.\n",
    "\n",
    "2. **Two-Level Modeling:**\n",
    "   - Stacking involves a two-level modeling process.\n",
    "   - In the first level, multiple base-level models are trained independently on the original training dataset.\n",
    "   - In the second level, a meta-model is trained using the predictions (outputs) from the base-level models as input features.\n",
    "\n",
    "3. **Base-Level Models:**\n",
    "   - Base-level models can be of various types, such as decision trees, random forests, support vector machines, or any other suitable model.\n",
    "   - Each base-level model is trained to make predictions on the target variable.\n",
    "\n",
    "4. **Meta-Model:**\n",
    "   - The meta-model is usually a simple model like logistic regression for classification or linear regression for regression tasks.\n",
    "   - It takes the predictions made by the base-level models as input features and learns to make the final prediction.\n",
    "\n",
    "5. **Training Process:**\n",
    "   - Stacking involves training the base-level models on the original training set.\n",
    "   - Once the base-level models are trained, they make predictions on the same training set.\n",
    "\n",
    "6. **Feature Engineering:**\n",
    "   - The predictions from the base-level models serve as new features or input for the meta-model.\n",
    "   - This process effectively transforms the original dataset into a new feature space.\n",
    "\n",
    "7. **Combining Predictions:**\n",
    "   - The meta-model is trained using these transformed features as input and is optimized to make the final prediction.\n",
    "   - For classification tasks, the meta-model often uses the class probabilities from the base models as input.\n",
    "   - For regression tasks, the meta-model uses the base model predictions as input.\n",
    "\n",
    "8. **Performance Boost:**\n",
    "   - Stacking can significantly improve predictive performance by leveraging the diversity of base-level models.\n",
    "   - It allows the ensemble to capture complex relationships in the data that individual models might miss.\n",
    "\n",
    "9. **Hyperparameter Tuning:**\n",
    "   - Stacking may involve hyperparameter tuning for both base-level models and the meta-model to optimize the ensemble's performance.\n",
    "\n",
    "10. **Common Usage:**\n",
    "    - Stacking is used in machine learning competitions and real-world applications where squeezing the last bit of predictive accuracy is essential.\n",
    "\n",
    "<!-- ![stacking](images/stacking.png) -->\n",
    "\n",
    "---\n",
    "\n",
    "## Voting\n",
    "\n",
    "Voting is an ensemble learning technique that combines multiple classification or regression models via a majority vote or averaging. The following figure shows the voting process:\n",
    "\n",
    "<!-- ![voting](images/voting.png) -->\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
