{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector spcae model\n",
    "\n",
    "\n",
    "\n",
    "Vector spaces are fundamental in many applications in NLP. If you were to represent a word, document, tweet, or any form of text, you will probably be encoding it as a vector. These vectors are important in tasks like information extraction, machine translation, and chatbots. Vector spaces could also be used to help you identify relationships between words \n",
    "\n",
    "\n",
    "## Euclidean distance \n",
    "\n",
    "$$d(\\vec{v},\\vec{w}) = \\sqrt{\\sum_{i=1}^n (v_i - w_i)^2}$$\n",
    "\n",
    "where:\n",
    "- $d(\\vec{v},\\vec{w})$ represents the Euclidean distance between vectors $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "Issues with Euclidean distance:\n",
    "One of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric. For example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result. Look at the diagram below:\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/issue_with_euclidian_distance.png\" alt=\"Alt text\" width=\"500\" height=\"200\" />\n",
    "</div>\n",
    "\n",
    "## Cosine Similarity\n",
    "Before getting into the cosine similarity function remember that the norm of a vector is defined as:\n",
    "\n",
    "$$\\|\\vec{v}\\| = \\sqrt{\\sum_{i=1}^n v_i^2}$$\n",
    "\n",
    "The cosine similarity function is defined as:\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{w}}{\\|\\vec{v}\\| \\|\\vec{w}\\|}$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ is the angle between vectors $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "\n",
    "Principal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data. As a result, it allows you to visualize your data. It tries to combine variances across features. \n",
    "\n",
    "Note that when doing PCA on this data, you will see that oil & gas are close to one another and town & city are also close to one another. To plot the data you can use PCA to go from d>2 dimensions to d=2. \n",
    "\n",
    "Eigenvector: the resulting vectors, also known as the uncorrelated features of your data\n",
    "\n",
    "Eigenvalue: the amount of information retained by each new feature. You can think of it as the variance in the eigenvector. \n",
    "\n",
    "Also each eigenvalue has a corresponding eigenvector. The eigenvalue tells you how much variance there is in the eigenvector. Here are the steps required to compute PCA: \n",
    "\n",
    "Steps to Compute PCA: \n",
    "  - Mean normalize your data \n",
    "  - Compute the covariance matrix \n",
    "  - Compute SVD on your covariance matrix. This returns [USV]=svd(Î£). The three matrices U, S, V are drawn above. U is labelled with eigenvectors, and S is labelled with eigenvalues. \n",
    "  - You can then use the first n columns of vector U, to get your new data by multiplying XU[:,0:n]. \n",
    "\n",
    "  Steps to Compute PCA:\n",
    "\n",
    "  $$\\Sigma = \\frac{1}{m}X^TX$$\n",
    "\n",
    "  $$[U,S,V] = svd(\\Sigma)$$\n",
    "\n",
    "  $$X_{new} = XU[:,0:n]$$\n",
    "\n",
    "  where:\n",
    "  - $X$ is the data matrix\n",
    "  - $m$ is the number of samples\n",
    "  - $n$ is the number of features\n",
    "  - $U$ is the eigenvector matrix\n",
    "  - $S$ is the eigenvalue matrix\n",
    "  - $V$ is the eigenvector matrix\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "usa  = (5,6) \n",
    "wash = (10,5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.306623862918075, 0.08512565307587484)\n",
      "(5.477225575051661, -0.1386750490563073)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eu_dist(v, w):\n",
    "    # return \n",
    "    return np.linalg.norm(v-w),np.dot(v, w) / (np.linalg.norm(v) * np.linalg.norm(w))\n",
    "\n",
    "usa = np.array([5, 6])\n",
    "wash = np.array([10, 5])\n",
    "ag = np.array([9,1])\n",
    "X = usa - wash + ag\n",
    "# print(eu_dist(usa,wash)) \n",
    "# print(eu_dist(X,np.array([3,1]))) \n",
    "# print(eu_dist(X,np.array([4,3]))) \n",
    "# print(eu_dist(X,np.array([5,5]))) \n",
    "\n",
    "v = np.array([1,0,-1])\n",
    "w = np.array([2,8,1])\n",
    "X = np.array([3,1,4])\n",
    "print(eu_dist(v, w))\n",
    "print(eu_dist(X, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_pca(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: of dimension (m,n) where each row corresponds to a word vector\n",
    "        n_components: Number of components you want to keep.\n",
    "    Output:\n",
    "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
    "    pass in: data as 2D NumPy array\n",
    "    \"\"\"\n",
    "    # mean center the data\n",
    "    X_demeaned = X - np.mean(X,axis=0)\n",
    "\n",
    "    # calculate the covariance matrix\n",
    "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
    "    \n",
    "    # calculate eigenvectors & eigenvalues of the covariance matrix \n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # sort eigenvalue in increasing order (get the indices from the sort)\n",
    "    idx_sorted = np.argsort(eigen_vals)\n",
    "    \n",
    "    # reverse the order so that it's from highest to lowest. \n",
    "    idx_sorted_decreasing = idx_sorted[::-1]\n",
    "    \n",
    "    # sort the eigen values by idx_sorted_decreasing\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "    \n",
    "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
    "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n",
    "\n",
    "    # select the first n eigenvectors (n is desired dimension \n",
    "    # of rescaled data array, or n_components)\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n",
    "\n",
    "    # transform the data by multiplying the transpose of the eigenvectors with the transpose of the de-meaned data\n",
    "    # Then take the transpose of that product.\n",
    "    # transform the data \n",
    "    X_reduced = np.dot(eigen_vecs_subset.transpose(), X_demeaned.transpose()).transpose()\n",
    "\n",
    "    return X_reduced\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
