{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,nlayers: int, dropout: float = 0.5):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            ntoken: the size of vocabulary\n",
    "            d_model: the number of expected features in the encoder/decoder inputs\n",
    "            nhead: the number of heads in the multiheadattention models\n",
    "            d_hid: the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "            nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "            dropout: the dropout value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "        self.use_nested_tensor = True\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is mps\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torch.utils import data\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"device is\", device)\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/ml_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 83.37 | loss  7.56 | ppl  1926.63\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 62.16 | loss  6.93 | ppl  1021.98\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 62.23 | loss  6.61 | ppl   739.98\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 62.06 | loss  6.53 | ppl   683.27\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 62.27 | loss  6.41 | ppl   610.26\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 62.91 | loss  6.42 | ppl   611.20\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 62.00 | loss  6.37 | ppl   581.44\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 62.35 | loss  6.35 | ppl   574.56\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 61.81 | loss  6.27 | ppl   530.70\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 62.19 | loss  6.29 | ppl   540.55\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 62.50 | loss  6.16 | ppl   475.69\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 62.57 | loss  6.24 | ppl   511.24\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 62.17 | loss  6.23 | ppl   509.42\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 62.57 | loss  6.18 | ppl   482.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 197.97s | valid loss  5.99 | valid ppl   397.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 62.97 | loss  6.14 | ppl   465.32\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 62.25 | loss  6.15 | ppl   466.50\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 62.09 | loss  6.01 | ppl   407.32\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 62.28 | loss  6.06 | ppl   426.92\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 62.77 | loss  6.02 | ppl   409.59\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 62.18 | loss  6.05 | ppl   424.34\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 62.02 | loss  6.05 | ppl   424.71\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 62.38 | loss  6.07 | ppl   432.48\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 61.90 | loss  6.02 | ppl   411.16\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 62.08 | loss  6.04 | ppl   421.17\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 61.98 | loss  5.94 | ppl   381.58\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 62.20 | loss  6.02 | ppl   411.08\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 62.31 | loss  6.03 | ppl   414.21\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 62.08 | loss  5.98 | ppl   394.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 191.68s | valid loss  5.90 | valid ppl   366.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 62.30 | loss  5.98 | ppl   395.29\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 61.93 | loss  5.99 | ppl   401.31\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 62.11 | loss  5.86 | ppl   351.02\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 61.88 | loss  5.92 | ppl   370.78\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 62.09 | loss  5.88 | ppl   358.36\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 62.00 | loss  5.92 | ppl   371.83\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 61.92 | loss  5.92 | ppl   373.59\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 61.92 | loss  5.94 | ppl   380.15\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 61.98 | loss  5.90 | ppl   365.53\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 61.93 | loss  5.92 | ppl   374.02\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 62.04 | loss  5.81 | ppl   334.34\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 61.95 | loss  5.90 | ppl   365.09\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 62.17 | loss  5.91 | ppl   367.37\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 62.03 | loss  5.86 | ppl   350.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 190.99s | valid loss  5.85 | valid ppl   348.25\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.77 | test ppl   320.72\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', '<unk>', 'chronicles', '(', 'japanese', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a']\n",
      "torch.Size([35, 20, 28782]) torch.Size([700, 28782]) torch.Size([700])\n",
      "[',', 'yard', 'to', ',', 'of', ',', ',', ',', 'the', 'the', 'the', ',', 'to', ',', ',', ',', 'first', '<unk>', 'average', ',', ',', ',', 'the', 'a', ',', 'the', 'a', ',', ',', ',', 'to', 'the', 'the', 'the', 'is', 'the', ',', 'a', 'in', 'the', ',', ',', ',', 'in', 'the', 'it', 'in', 'the', 'the', ',', 'the', 'was', 'the', '@-@', 'the', ',', 'to', ',', 'yard', ',', '=', ',', ',', 'in', 'election', ',', 'the', '<unk>', 'the', 'the', ',', 'a', 'side', '0', 'to', 'the', 'been', 'the', ',', ',', 'of', '1999', ',', 'the', 'the', 'a', '<unk>', 'in', 'was', ',', ',', 'in', ',', ',', ',', ',', 'a', '=', '@-@', '<unk>', 'longer', '<unk>', 'the', ',', ',', 'a', ',', 'century', '<unk>', ',', 'the', 'the', 'the', 'the', 'the', '<unk>', 'to', '=', '@-@', ',', ',', ')', 'was', 'in', 'been', 'the', ',', ',', '.', 'the', 'a', ',', ',', 'second', 'was', ',', 'the', ',', ',', 'the', '–', 'the', ',', 'a', 'in', 'the', 'the', '<unk>', ',', '<unk>', 'as', 's', 'the', 'of', 'a', 'the', ',', '=', 'the', 'first', ',', ')', 'the', 'side', '<unk>', ',', ',', ',', 'of', ',', 'as', 'first', ',', 'the', ',', 'of', 'first', '=', 'to', ',', ',', 'the', '<unk>', 'the', '<unk>', 's', 'of', '<unk>', 'the', ',', 'the', ',', 'the', 'the', 'the', ',', ',', '=', 'the', ',', '0', ')', 'of', 'second', ',', 'first', 'the', '19th', 'to', 'the', '<unk>', 'the', 'the', 'second', ',', ',', 'the', 'been', ',', 'the', '@-@', ',', 'the', ',', 'the', ',', 'kakapo', ',', 'the', 'was', ',', '<unk>', 'end', 'one', 'the', 'in', 'the', '<unk>', 'the', '<unk>', ')', ',', ',', '<unk>', '<unk>', ',', ',', 'of', '<unk>', ',', '<unk>', 'to', 'of', 'scored', '<unk>', 'the', '<unk>', ',', 'the', 'are', 'the', 'the', '<unk>', 'the', ',', '<unk>', 'he', 'the', ',', 'a', 'included', ',', 'the', ',', ',', ',', 'of', 'the', '@-@', 'be', ',', 'he', ',', 'in', 'of', ',', 'was', ',', ',', 'on', 'the', 'are', 'kakapo', 'the', 's', 'the', 'the', '@-@', 'of', ',', 'the', ',', ',', 'the', 'the', 'was', 'been', ',', 'the', 'the', 'first', 'no', ',', 'second', '<unk>', 'the', ',', '.', 'the', '<unk>', ')', 'the', 'the', ',', 'kakapo', 'in', ',', '<unk>', ',', '<unk>', ',', 'longer', ',', 'quarter', 'to', 'the', 'of', '@', 'was', ',', 'the', \"'\", ',', 'the', ',', ',', 'the', ',', '<unk>', ',', ',', ',', 'the', 'on', 'of', 'one', ',', '.', 'that', 'the', 'kakapo', 'a', 'the', 'end', 'the', 'to', 'kakapo', ',', '@-@', 'first', 'the', 'the', ',', 'the', ',', 'end', ',', 'in', 'first', 'time', ')', 'second', ',', ',', 'of', 'the', 'year', ',', ',', 'that', 'first', ',', 'been', 'touchdown', 'the', ',', '<unk>', 'in', ',', ',', '@', 'one', 'the', ',', ',', ',', 'in', '<unk>', 'the', 'she', ',', 'the', 'that', '@-@', 'was', 'the', ',', ',', 'a', '<unk>', '=', 'in', '<unk>', 'the', '<unk>', 'the', 'the', ',', '<unk>', 'was', ',', 's', 'he', 'yard', 'be', ',', ',', ',', 'he', ',', 'the', ',', 'zealand', '1', '<unk>', '<unk>', 'state', '<unk>', ',', 'be', 'the', ',', 'was', 'pass', 'place', 'in', ',', 'the', \"'\", ',', ',', 'the', ',', 'million', 'the', ',', ',', ',', 'the', ',', ',', 'the', 'a', ',', ',', 'end', '<unk>', 'to', 'a', 'the', 'to', ',', 'a', '.', 'second', 'the', 'a', ',', 'the', 'the', 'yard', 's', ',', 'goal', ',', 'was', 'of', 'the', '<unk>', 'feelings', 'the', 'the', ',', '@', 'century', ',', 'to', 'the', ',', ',', ',', ',', 'the', ',', 'the', 'of', ',', 'first', ',', 'of', 'the', '@-@', 'the', '.', ',', '<unk>', 'the', '<unk>', ',', 'the', ',', 'the', 'first', 'the', ',', 'a', '<unk>', 'to', ',', 'it', '@', '@', ',', 'million', ',', ',', 'in', '<unk>', 'the', 'time', 'of', 'first', ',', ',', 'first', ',', '<unk>', 'to', 'the', 'the', ',', ',', 'the', ',', 'the', 'the', 'to', 'of', 'a', ',', ',', '<unk>', 's', 'to', ',', 'first', ',', '@', ',', ',', ',', 'the', 'are', '@', 'been', ',', 'time', 'a', 'of', ',', 'the', 's', 'first', 'second', 'the', 'minister', ',', 'the', ',', ',', 'the', ',', 'no', 'the', 'a', 'the', 'time', ',', 'of', 'the', ',', ',', ',', ',', ',', ',', 'on', 'time', 'be', ',', ',', 'the', 'average', 'second', 'kakapo', ',', 'in', 'the', ',', 'show', 'the', 's', 'his', 'the', 'a', 'a', ',', ',', 'been', 'the', 'the', ',', ',', ',', 'on', 's', 'a', '<unk>', '<unk>', 'of', '<unk>', 'the', ',', '@-@', 'of', 'the', 'the', '=', 'it', 'it', 'a', 'the', 'the', ',', 'the', 'first', 'to', 'of', ',', ',', 'league', ',', 'a', '0', 'the', 'to', 'average', '=', 'was', \"'\", '<unk>', 'to', '<unk>', 'in', 's', ',', 'the', 'the', 'been', 'of', ',', 'the', ',', '@-@', 'children', 'the', 'to', '0', 'a', 'time']\n",
      "['chronicles', 'star', 'by', 'were', 'culture', ',', 'was', 'dates', 'banksia', 'behead', 'according', '.', 'on', 'to', 'it', 'for', 'skeptical', 'being', 'index', 'of', 'iii', 'game', 'henry', 'shut', 'in', 'but', 'tackled', 'from', ',', 'him', 'to', 'hornung', 'for', '24', 'as', 'zoos', 'scully', 'trapped', '@-@', 'things', '=', 'history', 'mitchell', 'down', 'general', 'janelle', 'for', 'the', 'giving', ',', '<unk>', 'was', 'either', '–', 'familiar', ',', 'has', '.', 'linked', 'spiritual', 'senjō', '(', 'barnes', 'as', '.', 'was', 'a', 'early', 'it', 'macleod', 'literature', 'angered', 'boat', '7', 'territory', 'museums', 'been', '=', 'annual', 'and', 'no', 'a', ',', 'traffic', 'religion', 'not', 'big', '16th', 'the', 'jumped', ',', 'by', 'to', 'at', ',', 'and', 'assigned', '=', 'fixed', 'material', 'valkyria', 'record', 'who', 'lights', 'has', '.', 'loss', 'century', 'specific', 'on', 'was', 'doyle', 'win', 'the', 'she', 'collectors', 'to', 'geology', 'royalty', 'with', '3', 'since', 'settled', 'were', 'played', 'although', ',', 'and', 'epithet', 'a', 'well', \"'\", '.', 'half', 'was', '.', '<unk>', '=', ',', 'his', '<unk>', 'broken', 'near', 'either', 'a', 'christine', 'losing', 'shows', 'violacea', '<unk>', 'known', 's', 'however', '.', 'hesitant', 'most', 'his', '=', 'estimated', 'characteristic', 'chronicles', 'by', 'the', 'on', 'significant', \"'\", 'some', 'a', 'in', '<unk>', 'as', 'action', ',', 'with', 'about', 'captured', 'work', 'ceres', 'by', 'earnestness', '(', 'kobe', 'junction', 'the', 'role', 's', 'of', 'late', 'reference', 'on', 'a', ',', 'as', 'the', 'committing', 'specimens', '.', 'has', 'lovell', ',', 'japanese', 'bryant', 'of', 'ground', 'in', 'mother', 'the', '<unk>', 'to', 'which', 'port', 'and', 'the', 'only', 'to', 'died', 'in', 'a', 'at', 'and', '戦場のヴァルキュリア3', ')', 'union', 'or', 'the', 'left', 'yardage', 'style', 'the', 'tessa', 'and', 'told', 'start', 'alabama', 'the', 'within', 'the', 'mass', 'around', 'we', ',', '.', 'and', 'without', 'cultural', 'the', 'that', 'with', 'violet', 'was', 'also', 'him', 'of', 'points', 'show', 'months', 'episode', 'of', '15', 'can', 'lit', 'that', 'taylor', 'power', 'life', 'faith', 'he', 'flying', 'flowers', 'working', 'for', 'there', 'the', 'in', \"'\", '.', ',', '9', '%', '<unk>', '.', 'year', 'creeks', '.', 'of', 'she', 'had', 'buttresses', '.', ',', 'its', 'was', 'race', 'the', 's', 'from', 'assistant', '@', '.', 'the', 'valkyria', ',', 'in', 'damages', 'the', 'still', 'gained', 'and', 'thus', 'and', 'salt', 'no', 'approached', 'third', 'demanding', 'at', 'director', '.', 'mosley', 'characteristics', 'of', 'jordan', '1867', 'on', 'island', 'supports', 'with', '<unk>', 'the', 'charmed', 'pans', 'need', ',', 'scored', 'shooting', 'least', 'walter', '@', 'said', 'of', 'the', 'was', '.', 'the', 'since', 'them', 'the', 'pinnacles', 'full', 'her', 'during', 'for', 'williams', 'on', 'schedule', 'the', 'skinner', '39', 'my', 'this', 'battlefield', 'the', 'growth', 'island', 'ancient', '.', 'previous', '<unk>', 'name', 'so', 'the', 'him', 'had', 'a', 'as', '1870s', '(', '×', 'belief', 'acute', '3', 'only', 'of', 'totaled', 'times', 'months', 'play', 'a', 'of', 'that', '<unk>', 'to', 'suggested', '32', 'it', ',', 'mitch', '<unk>', 'is', 'and', ')', 'washington', 'the', 'to', '(', 'before', '.', '<unk>', 'the', 'he', 'dynasty', \"'\", 'that', '@-@', 'would', 'collectors', 'pileggi', 'kg', 'that', 'sympathetic', ',', 'player', 'new', '$', 'and', 'the', 'nc', 'and', 'species', 'could', '.', 'butt', 'he', 'yard', 'take', 'knew', ')', 'as', 'i', 'explorer', 'commonly', 'to', 'settlement', '5', 'since', 'marriage', 'state', 'pierced', ',', 'stay', 'arikamedu', 'in', 'was', 'shelley', 'time', 'the', 'and', 'determined', 'got', 'of', 'referred', 'play', 'was', '@', 'the', 'of', 'was', 'parapet', 'with', 'on', '@-@', \"'\", 'worried', 'field', 'away', 'kakapo', 'michael', 'from', 'a', 'human', 'to', 'in', 'rapid', '.', '17th', 'janelle', 'unable', '.', 'author', 'board', '<unk>', 'except', 'by', 'goal', 'from', 'population', 'kritschgau', 'the', 'better', 'nature', 'as', 'all', 'in', '@', 'century', 'and', 'to', 'the', 'citation', '.', 'together', 'for', 'his', ',', 'raising', 'was', '(', 'dawn', 'deal', '.', 'valkyria', '82', 'particular', '3', 'plantations', 'kody', 'pick', 'medieval', ',', 'another', 'find', 'his', 'crew', 'auburn', 'her', 'declining', 'john', 'spacecraft', 'than', 'in', 'chronicles', 'games', ',', 'million', ',', ',', 'up', 'era', 'is', 'flashback', 'mention', 'own', \"'\", 'brought', 'son', 'their', 'finn', '.', 'anyone', '1928', 'iii', ',', 'there', '.', 'has', 'however', 'another', 'is', 'banksia', 'scene', 'as', \"'\", 's', 'the', '.', 'prime', ')', 'with', 'else', 'priestley', 'outside', 'starting', 'was', 'in', 'been', ',', 'first', 'represented', 'violacea', 'in', 'poduke', 'satisfaction', 'inconsistency', 'margin', 'kyra', 'concern', 'work', 'this', 'could', 'observed', 'japan', 'in', 'an', 'the', 'the', 'janelle', 'down', 'by', '<unk>', 'the', ',', \"'\", 'and', 'to', 'was', 'was', 'desperately', 'mass', 'have', ',', ',', '67', 'influx', '<unk>', 'focus', \"'\", 'and', 'the', 'the', 'episode', 'a', '.', 'oxford', '27', 'one', 'to', 'in', 'ceres', 'because', 'when', 'is', 'of', 'of', '<unk>', 'of', 's', 'sent', 'remains', 'species', 'counterfeit', 'major', 'relations', 'were', '–', 'of', 'collect', 'an', 'comprises', 'it', 'i', 'a', 'them', 'german', 'quarter', 'political', 'mother', 'in', 'of', 'has', 'part', 'port', 'between', 'declared', '21', 'three', 'as', 'attempt', 'approximately', 'was', 'first', 'tactical', '.', 'families', 'on', 'identity', 'entered', 'mike', 'the', 'been', 'two', 'in', 'the', 'favourites', 'entering', 'characters', 'many', 'to', 'a', 'more', 'remember']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # for i in range(0, test_data.size(0) - 1, bptt):\n",
    "    data, targets = get_batch(train_data, 1)\n",
    "    seq_len = data.size(0)\n",
    "    print(vocab.lookup_tokens(data[:, 0].tolist()))\n",
    "    output = model(data)\n",
    "    \n",
    "    # print(output)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    print(output.shape,output_flat.shape, targets.shape,)\n",
    "    print(vocab.lookup_tokens(torch.argmax(output_flat, dim=1).tolist()))\n",
    "    print(vocab.lookup_tokens(targets.tolist()))\n",
    "    # print(output_flat)\n",
    "    # total_loss += seq_len * criterion(output_flat, targets).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
