{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "Part of Speech Tagging (POS) is the process of assigning a part of speech to a word. By doing so, you will learn the following: \n",
    "\n",
    "Markov Chains\n",
    "Hidden Markov Models\n",
    "Viterbi algorithm\n",
    "\n",
    "You can use part of speech tagging for: \n",
    "\n",
    "Identifying named entities\n",
    "Speech recognition\n",
    "Coreference Resolution\n",
    "\n",
    "## Markov Chains\n",
    "\n",
    "You can use Markov chains to identify the probability of the next word based on current state . For example below, you can see that the most likely word after a verb is a noun. \n",
    "The circles of the graph represent the states of your model. A state refers to a certain condition of the present moment.  You can think of these as the POS tags of the current word.\n",
    "\n",
    "## states Q\n",
    "$$ Q = {q_1, q_2, q_3} $$ \n",
    "\n",
    "is the set of all states in your model. \n",
    "\n",
    "## Transition Probabilities\n",
    "In more general notation, you can write the transition matrix A, given some states Q, as follows: \n",
    "\n",
    "$$ A = a_{ij} = P(t_{i}| t_{i-1}) $$\n",
    "\n",
    "The transition matrix A is a matrix of size $|Q| \\times |Q|$. \n",
    "\n",
    "Count of times tag (i-1) shows up before tag i: $ C(t_{i-1}, t_{i}) $\n",
    "\n",
    "Probability of going from tag (i-1) to tag i: \n",
    "$$ P(t_{i}| t_{i-1}) = \\frac{C(t_{i-1}, t_{i})}{\\sum_{j=1}^{N}C(t_{i-1}, t_{j})} $$\n",
    "\n",
    "Unfortunately, sometimes you might not see two POS tags in front each other. This will give you a probability of 0. To solve this issue, you will \"smooth\" it as follows: \n",
    "\n",
    "$$ P(t_{i}| t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha}{\\sum_{j=1}^{N}C(t_{i-1}, t_{j}) + \\alpha N} $$\n",
    "\n",
    "where N is the number of POS tags in your tag set.\n",
    "\n",
    "## Emission Probabilities\n",
    "\n",
    "Count associated with how many times the tag $t_{i}$ is associated with the word $w_{i}$: $ C(t_{i}, w_{i}) $\n",
    "\n",
    "Probability of a word given a tag:\n",
    "$$ P(w_{i}| t_{i}) = \\frac{C(t_{i}, w_{i})}{\\sum_{j=1}^{N}C(t_{i}, w_{j})} $$\n",
    "Again, you will need to smooth this probability as well.\n",
    "$$ P(w_{i}| t_{i}) = \\frac{C(t_{i}, w_{i}) + \\alpha}{\\sum_{j=1}^{N}C(t_{i}, w_{j}) + \\alpha N} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Hidden Markov Models\n",
    "In the previous video, I showed you an example with a simple markov model. The transition probabilities allowed you to identify the transition probability from one POS to another. We will now explore hidden markov models. In hidden markov models you make use of emission probabilities that give you the probability to go from one state (POS tag) to a specific word. \n",
    "\n",
    "\n",
    "## The Viterbi Algorithm \n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM).\n",
    "\n",
    "- Initialization\n",
    "\n",
    "    You will now populate a matrix C of dimension (num_tags, num_words). This matrix will have the probabilities that will tell you what part of speech each word belongs to. \n",
    "\n",
    "    The goal is to find the most likely sequence of part-of-speech (POS) tags for a given sequence of words $w_1, w_2, \\dots, w_K$ using a hidden Markov model. \n",
    "\n",
    "    We build up a matrix $C$ that contains the probabilities of being in each POS tag at each word position. The first column $C_{:,1}$ contains the probabilities of being in each tag after seeing just the first word $w_1$.\n",
    "\n",
    "    To populate this first column, we take the initial POS tag probabilities $\\pi_i$ and multiply them by the emission probability $b_{i,j}$ of generating the first word $w_1$ from each tag $i$. The emission probability is indexed by $cindex(w_1)$ which maps the first word $w_1$ to its position in the emission matrix:\n",
    "\n",
    "    $$C_{i,1} = \\pi_i b_{i, cindex(w_1)}$$\n",
    "\n",
    "    We also track the previous POS tag using a matrix $D$. This is initialized to 0 for the first column since there is no previous tag yet: \n",
    "\n",
    "    $$D_{:,1} = 0$$\n",
    "\n",
    "    The $C$ matrix is populated by taking the product of the initial and emission probabilities, while the $D$ matrix keeps track of the previous tag at each position to reconstruct the full sequence later.\n",
    "\n",
    "- Forward Pass\n",
    "\n",
    "    The forward pass iterates through the words $w_2, w_3, \\dots, w_K$ and fills in the remaining columns of $C$ and $D$. The probability of being in each tag $j$ at position $k$ is the sum of the probabilities of all paths that could reach that state. Each path is the product of the probability of being in each tag $i$ at the previous position $k-1$ and the probability of transitioning from tag $i$ to tag $j$.\n",
    "\n",
    "Here are the combined formulas and steps for the forward variables (C matrix) and backpointer variables (D matrix) in the forward algorithm for HMMs:\n",
    "\n",
    "**Forward Variables (C matrix):**\n",
    "\n",
    "$$c_{i,j} = \\max_{1 \\leq k \\leq N}(c_{k,j-1}a_{k,i})b_{i,o_j}$$ \n",
    "\n",
    "**Backpointer Variables (D matrix):**\n",
    "  \n",
    "$$d_{i,j} = \\argmax_{1 \\leq k \\leq N}(c_{k,j-1}a_{k,i})$$\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Initialization: \n",
    "   ```\n",
    "   c_{i,1} = π_ib_{i,o_1}  \n",
    "   d_{i,1} = 0\n",
    "   ```\n",
    "\n",
    "2. Recursion:\n",
    "   ```\n",
    "   For j = 2 to T:\n",
    "     For i = 1 to N: \n",
    "       c_{i,j} = max_k (c_{k,j-1}a_{k,i})b_{i,o_j}  \n",
    "       d_{i,j} = argmax_k (c_{k,j-1}a_{k,i})\n",
    "   ```\n",
    "   \n",
    "So in summary, the C matrix calculates the forward probability using the previous forward probability, transition probability, and emission probability through a recursion. The D matrix calculates the argmax of the previous forward probability and transition probability, storing backpointers for optimal state sequence reconstruction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
