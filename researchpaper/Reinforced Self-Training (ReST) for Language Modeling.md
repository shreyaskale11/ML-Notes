# Reinforced Self-Training (ReST) for Language Modeling

As an AI model is trained on a dataset, it can become overconfident in its predictions and produce low-quality outputs. Reinforced Self-Training (ReST) is a new algorithm that addresses this issue by using a combination of self-training and reinforcement learning to improve the quality of the model's predictions. ReST works by generating synthetic data from the model's own predictions, and then using this data to train the model further. The algorithm also uses a reward function to encourage the model to produce high-quality outputs. The authors of this paper demonstrate that ReST can substantially improve translation quality in a compute and sample-efficient manner, and that it outperforms other state-of-the-art machine translation models in both automated metrics and human evaluation. They also suggest that ReST could be applied to other generative learning settings besides machine translation.

## Questions 
How does ReST differ from typical online RLHF methods, and why is it more efficient?

ReST differs from typical online RLHF methods in that it produces a training dataset offline, which allows for data reuse and makes it more computationally efficient. In contrast, online methods require sampling from the updated policy and scoring the samples with the reward model many times during training, which can be computationally expensive. Additionally, online methods are prone to reward "hacking" and require model regularization to mitigate this issue. ReST, on the other hand, uses offline RL methods that learn from a fixed dataset of examples and are less prone to reward hacking. This makes ReST more efficient than typical online RLHF methods.
Conditional Language Model: This is a type of language model that generates output sequences (denoted as ùíö) based on a given context or source input sequence (denoted as ùíô). Both the input and output sequences consist of tokens from a chosen vocabulary.

How does ReST generate synthetic data to train the model further?

ReST generates synthetic data by using the current policy to generate new examples. During the "Grow" step, the current policy generates a large dataset of synthetic examples. These examples are then filtered based on their quality using a reference-free reward model. The remaining high-quality examples are used to fine-tune the policy during the "Improve" step. This process is repeated, with the Improve step being repeated more frequently to amortize the dataset creation cost. By generating synthetic data from the model's own predictions and using it to train the model further, ReST aims to improve the quality of the model's predictions.

How did the authors test the quality of the rewards generated for ReST?

The authors tested the quality of the rewards generated for ReST by conducting a series of experiments. First, they used a "unit test" methodology to evaluate the reward model's ability to distinguish between good and bad translations. In this methodology, they created a set of synthetic examples with known quality and evaluated the reward model's predictions on these examples. They found that the reward model was able to accurately distinguish between good and bad translations.

Next, they evaluated the quality of the rewards generated by the reward model on a held-out validation set. They compared the performance of ReST to several other state-of-the-art machine translation models and found that ReST outperformed the other models on this validation set. Finally, they conducted a human evaluation of the translations generated by ReST and found that the translations were of high quality and comparable to those generated by human translators.

How reward models were trained?

The authors of the paper used learned reward models to assign a score to the whole translation. They considered two types of reward models for translation: reference-free reward models and reference-based reward models. The reference-free reward models estimate how good a translation is based on the source and candidate sentences only, while the reference-based reward models additionally use the reference human translation to decide how good a candidate translation is. In their experiments, the authors chose to work with reference-free reward models . 

The reward models were trained using a combination of supervised and reinforcement learning. Specifically, the authors used a combination of maximum likelihood estimation and policy gradient methods to train the reward models. During training, the reward models were optimized to assign high scores to high-quality translations and low scores to low-quality translations. The authors also used a technique called "adversarial training" to make the reward models more robust to distribution shifts in the candidates generated by the model and reward hacking .

What were the results of the human evaluation of ReST's translations?

The authors conducted a human evaluation of the translations generated by ReST to investigate if ReST can outperform the baseline model (BC) in human evaluations. They displayed a sentence in the source language and two generated translations: one by BC model (G=0 I=0) and one by a ReST variant. Human raters scored each translation on a scale from 0 to 6, and the authors measured the difference between the average score of ReST method and of BC which they referred to as "Human eval diff". In Figure 7 (right) of the paper, the authors reported that all variants of ReST outperformed the BC baseline significantly in terms of human evaluation. Specifically, the "Human eval diff" for ReST was higher than that of BC, indicating that ReST generated translations that were preferred by human raters over those generated by the baseline model .

<div align="center">
  <img src="images\ReST-performance based on HF.png" alt="Alt text"  height="250" />
</div>

What are some potential applications of ReST beyond machine translation?

ReST can be applied to many tasks within the language domain beyond machine translation, such as summarization, turn-based dialogue, and other generative audio and video models. The authors of the paper conclude that ReST is a general and efficient approach that can be applied when a robust reward model of human preferences is available and when we are able to generate samples from the model at scale. They believe that ReST is a useful growing batch RL methodology for RLHF (Reinforcement Learning for Human Feedback) with several avenues for future exploration and applications .

## Summary
images\ReST-performance based on HF.png
![Alt text](images\ReST-conditional probability distribution.png)

<div align="center">
  <img src="images\ReST-conditional probability distribution.png" alt="Alt text"  height="200" />
</div>

Sequential Generation: Auto-regressive models generate sequences of data (e.g., text, speech, time series) one element at a time, where each element depends on the elements that came before it in the sequence.

Conditional Probability: At each time step, the model calculates a conditional probability distribution over the possible values for the next element in the sequence. This distribution is conditioned on the elements generated in the previous time steps.




## References
Reinforced Self-Training (ReST) for Language Modeling (https://arxiv.org/pdf/2308.08998.pdf)




<!--  -->